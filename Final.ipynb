{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"제출(기석).ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GEucXZ31A28N"},"source":["## 코드 설명에 앞서 (느낀점)\n","1. 케글에 유사대회에서 코드를 긁어와 극히 일부분만을 수정하였습니다. (https://www.kaggle.com/c/spooky-author-identification)\n","\n","2. nn, cnn, gru 각각의 부분에서 최고의 정확도를 낼 수 있게 수정해주었더니 특정 feature가 과도하게 weight가 들어가면서 오히려 전체 stacking enesmble에서 더 저조한 성적을 보이는 현상이 일어났습니다. 결국엔 어쩔수 없이 수정이 없는 원본이 최종 제출점수가 되어버렸습니다.\n","\n","3. GPU 사용을 위해 google colab을 사용하였습니다. (하지만 RNN부분에서는 큰 속도향상을 기대할 순 없었습니다)\n","\n","4. 케글 유사대회에 있는 거의 대부분의 코드를 리뷰하고 실제로 돌려보면서 느낀 것은, 일정 이상의 점수를 내기 위해선 매우 많은 모델들의 앙상블은 필수였으며, 더욱더 높은 점수를 내기 위해서는 해당 데이터에 맞는 text 손질이였습니다. (NLP 1인자 BERT 제외)\n","\n","5. 다른 중요한 공모전들과 일정이 겹치다보니 많은 시간을 투자할 수 없어서 데이터와 충분히 친해지지 못한 것이 아쉽고, 실제로 코드에도 그 모습이 고스란히 보이는 듯 합니다. 최적의 코드를 공유해드리지 못하여 아쉬움이 많이 남습니다.\n","\n","## 코드 공유에 앞서 (코드목차)\n","1. 데이터 전처리\n","\n","2. MultinomialNB을 이용한 feature 생성\n","\n","3. CNN을 이용한 feature 생성\n","\n","4. GRU를 이용한 feature 생성\n","\n","5. NN을 이용한 feature 생성\n","\n","6. 최종 stacking ensemble\n"]},{"cell_type":"code","metadata":{"id":"6uqg1URaP9km","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607330119488,"user_tz":-540,"elapsed":59621,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"1f4be5be-11b1-432a-b3bd-412708b3c5a5"},"source":["# 구글 드라이브에 마운트합니다.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pgZNpA45P9au","executionInfo":{"status":"ok","timestamp":1607330119494,"user_tz":-540,"elapsed":59606,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["#경로 설정\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/소설작가분류AI경진대회')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enSbARp2LWI8"},"source":["# 데이터 전처리"]},{"cell_type":"code","metadata":{"id":"VvTu2QW1PbHd","executionInfo":{"status":"ok","timestamp":1607330139826,"user_tz":-540,"elapsed":79926,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# libraries\n","import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test_x.csv\")\n","\n","# 단어수(중복 포함)\n","train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n","test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n","\n","# 단어수(중복 제거)\n","train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n","test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n","\n","# 글자수\n","train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n","test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n","\n","# stopwords : nltk의 stopwords보다 월등한 성능을 보여줍니다\n","stopwords = [\n","    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n","    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n","    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n","    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n","    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n","    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n","    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n","    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n","    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n","    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n","    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n","    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n","    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n","    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n","    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n","    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n","    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n","    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n","    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n","    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n","    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n","    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n","    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n","    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n","    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n","    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n","    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n","    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n","    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n","    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n","    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n","    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n","    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n","    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n","    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n","    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n","    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n","    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n","    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n","    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n","    \"yourselves\"]\n","\n","train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n","test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n","\n","# punctuation의 개수\n","import string\n","train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n","test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n","\n","# 대문자로만 이루어진 단어 개수\n","train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","\n","# 첫글자가 대문자인 단어 개수\n","train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n","test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n","\n","# text 평균 길이\n","train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"RE3sXFYqPbHg","colab":{"base_uri":"https://localhost:8080/","height":262},"executionInfo":{"status":"ok","timestamp":1607330139836,"user_tz":-540,"elapsed":79903,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"6b97b829-c336-4075-946f-b1406c6d86be"},"source":["train_df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>author</th>\n","      <th>num_words</th>\n","      <th>num_unique_words</th>\n","      <th>num_chars</th>\n","      <th>num_stopwords</th>\n","      <th>num_punctuations</th>\n","      <th>num_words_upper</th>\n","      <th>num_words_title</th>\n","      <th>mean_word_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>He was almost choking. There was so much, so m...</td>\n","      <td>3</td>\n","      <td>46</td>\n","      <td>39</td>\n","      <td>240</td>\n","      <td>27</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.239130</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>“Your sister asked for it, I suppose?”</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>38</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4.571429</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>She was engaged one day as she walked, in per...</td>\n","      <td>1</td>\n","      <td>57</td>\n","      <td>50</td>\n","      <td>320</td>\n","      <td>28</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.614035</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>The captain was in the porch, keeping himself ...</td>\n","      <td>4</td>\n","      <td>58</td>\n","      <td>49</td>\n","      <td>319</td>\n","      <td>27</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>4.517241</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n","      <td>3</td>\n","      <td>39</td>\n","      <td>36</td>\n","      <td>228</td>\n","      <td>16</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.871795</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index  ... mean_word_len\n","0      0  ...      4.239130\n","1      1  ...      4.571429\n","2      2  ...      4.614035\n","3      3  ...      4.517241\n","4      4  ...      4.871795\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"4Ob3Mx49PbHm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607330144420,"user_tz":-540,"elapsed":84468,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"2604160b-77aa-4fee-bbd6-b96974c85e89"},"source":["# Clean text\n","from tqdm import tqdm\n","tqdm.pandas()\n","punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n","def clean_text(x):\n","    x.lower()\n","    for p in punctuation:\n","        x.replace(p, '')\n","    return x\n","\n","train_df['text_cleaned'] = train_df['text'].apply(lambda x: clean_text(x))\n","test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))\n","\n","def extract_features(df):\n","    df['len'] = df['text'].apply(lambda x: len(x))\n","    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n","    df['n_.'] = df['text'].str.count('\\.')\n","    df['n_...'] = df['text'].str.count('\\...')\n","    df['n_,'] = df['text'].str.count('\\,')\n","    df['n_:'] = df['text'].str.count('\\:')\n","    df['n_;'] = df['text'].str.count('\\;')\n","    df['n_-'] = df['text'].str.count('\\-')\n","    df['n_?'] = df['text'].str.count('\\?')\n","    df['n_!'] = df['text'].str.count('\\!')\n","    df['n_\\''] = df['text'].str.count('\\'')\n","    df['n_\"'] = df['text'].str.count('\\\"')\n","\n","    # 문장 첫단어 개수\n","    df['n_The '] = df['text'].str.count('The ')\n","    df['n_I '] = df['text'].str.count('I ')\n","    df['n_It '] = df['text'].str.count('It ')\n","    df['n_He '] = df['text'].str.count('He ')\n","    df['n_Me '] = df['text'].str.count('Me ')\n","    df['n_She '] = df['text'].str.count('She ')\n","    df['n_We '] = df['text'].str.count('We ')\n","    df['n_They '] = df['text'].str.count('They ')\n","    df['n_You '] = df['text'].str.count('You ')\n","    df['n_the'] = df['text_cleaned'].str.count('the ')\n","    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n","    df['n_appear'] = df['text_cleaned'].str.count('appear')\n","    df['n_little'] = df['text_cleaned'].str.count('little')\n","    df['n_was '] = df['text_cleaned'].str.count('was ')\n","    df['n_one '] = df['text_cleaned'].str.count('one ')\n","    df['n_two '] = df['text_cleaned'].str.count('two ')\n","    df['n_three '] = df['text_cleaned'].str.count('three ')\n","    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n","    df['n_is '] = df['text_cleaned'].str.count('is ')\n","    df['n_are '] = df['text_cleaned'].str.count('are ')\n","    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n","    df['n_however'] = df['text_cleaned'].str.count('however')\n","    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n","    df['n_into'] = df['text_cleaned'].str.count('into')\n","    df['n_about '] = df['text_cleaned'].str.count('about ')\n","    df['n_th'] = df['text_cleaned'].str.count('th')\n","    df['n_er'] = df['text_cleaned'].str.count('er')\n","    df['n_ex'] = df['text_cleaned'].str.count('ex')\n","    df['n_an '] = df['text_cleaned'].str.count('an ')\n","    df['n_ground'] = df['text_cleaned'].str.count('ground')\n","    df['n_any'] = df['text_cleaned'].str.count('any')\n","    df['n_silence'] = df['text_cleaned'].str.count('silence')\n","    df['n_wall'] = df['text_cleaned'].str.count('wall')\n","\n","    df.drop(['text_cleaned'], axis=1, inplace=True)\n","\n","print('Processing train...')\n","extract_features(train_df)\n","print('Processing test...')\n","extract_features(test_df)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Processing train...\n","Processing test...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vk1hCvh_DhwX"},"source":["pos_tag와 ne_chunk를 이용한 tokenization. 자세한 내용은 https://statkclee.github.io/nlp2/nlp-ner-python.html 에 가면 확인 할 수 있다."]},{"cell_type":"code","metadata":{"id":"9l9rGMdcPbHn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607331525679,"user_tz":-540,"elapsed":1465705,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"d6a93f82-079b-41ec-e6bb-f6c6bb991d61"},"source":["import nltk\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('averaged_perceptron_tagger')\n","\n","def pos_tag_text(s):\n","    sents = nltk.sent_tokenize(s)\n","    res = []\n","    for sent in sents:\n","        words = nltk.word_tokenize(sent)\n","        tag_res = [a[1] for a in nltk.pos_tag(words)]\n","        res.append(' '.join(tag_res))\n","    return '. '.join(res)\n","\n","def ne_text(s):\n","    sents = nltk.sent_tokenize(s)\n","    res = []\n","    for sent in sents:\n","        words = nltk.word_tokenize(sent)\n","        tag_res = nltk.pos_tag(words)\n","        ne_tree = nltk.ne_chunk(tag_res)\n","        list_res = nltk.tree2conlltags(ne_tree)\n","        ne_res = [a[2] for a in list_res]\n","        res.append(' '.join(ne_res))\n","    return '. '.join(res)\n","\n","train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n","train_df['ne_txt'] = train_df[\"text\"].apply(ne_text)\n","test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n","test_df['ne_txt'] = test_df[\"text\"].apply(ne_text)\n","\n","c_vec3 = CountVectorizer(lowercase=False)\n","c_vec3.fit(train_df['tag_txt'].values.tolist())\n","train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n","test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n","print(train_cvec3.shape,test_cvec3.shape)\n","\n","c_vec4 = CountVectorizer(lowercase=False)\n","c_vec4.fit(train_df['ne_txt'].values.tolist())\n","train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n","test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n","print(train_cvec4.shape,test_cvec4.shape)\n","\n","tf_vec5 = TfidfVectorizer(lowercase=False)\n","tf_vec5.fit(train_df['tag_txt'].values.tolist())\n","train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n","test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n","print(train_tf5.shape,test_tf5.shape)\n","\n","tf_vec6 = TfidfVectorizer(lowercase=False)\n","tf_vec6.fit(train_df['ne_txt'].values.tolist())\n","train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n","test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n","print(train_tf6.shape,test_tf6.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","(54879, 34) (19617, 34)\n","(54879, 6) (19617, 6)\n","(54879, 34) (19617, 34)\n","(54879, 6) (19617, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mEVfNiqcLjnK"},"source":["# MultinomialNB을 이용한 feature 생성"]},{"cell_type":"code","metadata":{"id":"0lEBpxhqPbHn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607332034202,"user_tz":-540,"elapsed":1974207,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"368d0294-fa56-443a-c7ab-48e4c84f692f"},"source":["train_Y = train_df['author']\n","train_id = train_df['index'].values\n","test_id = test_df['index'].values\n","\n","# tfidf와 svd 합\n","tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n","full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist())\n","train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n","test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n","print(train_tfidf.shape,test_tfidf.shape)\n","\n","# svd1\n","n_comp = 30\n","svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n","svd_obj.fit(full_tfidf)\n","train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n","test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n","print(train_svd.shape,test_svd.shape)\n","\n","# tfidf char\n","tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n","full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist())\n","train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n","test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n","print(train_tfidf2.shape,test_tfidf2.shape)\n","\n","# svd2\n","n_comp = 30\n","svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n","svd_obj.fit(full_tfidf2)\n","train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n","test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n","print(train_svd2.shape,test_svd2.shape)\n","\n","\n","# cnt vec\n","c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n","c_vec.fit(train_df['text'].values.tolist())\n","train_cvec = c_vec.transform(train_df['text'].values.tolist())\n","test_cvec = c_vec.transform(test_df['text'].values.tolist())\n","print(train_cvec.shape,test_cvec.shape)\n","\n","# cnt char\n","c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n","c_vec2.fit(train_df['text'].values.tolist())\n","train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n","test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n","print(train_cvec2.shape,test_cvec2.shape)\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import log_loss\n","\n","feat_cnt = 5\n","\n","def gen_nb_feats(rnd=1):\n","    help_tfidf_train,help_tfidf_test = np.zeros((54879,5)),np.zeros((19617,5))\n","    help_tfidf_train2,help_tfidf_test2 = np.zeros((54879,5)),np.zeros((19617,5))\n","    help_cnt1_train,help_cnt1_test = np.zeros((54879,5)),np.zeros((19617,5))\n","    help_cnt2_train,help_cnt2_test = np.zeros((54879,5)),np.zeros((19617,5))\n","\n","    skf = StratifiedKFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n","    for train_index, test_index in skf.split(train_tfidf,train_Y):\n","        # tfidf to nb\n","        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n","        y_train, y_test = train_Y[train_index], train_Y[test_index]\n","        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n","        tmp_model.fit(X_train,y_train)\n","        tmp_train_feat = tmp_model.predict_proba(X_test)\n","        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n","        help_tfidf_train[test_index] = tmp_train_feat\n","        help_tfidf_test += tmp_test_feat/feat_cnt\n","\n","        # tfidf to nb\n","        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n","        tmp_model = MultinomialNB(0.025,fit_prior=False)\n","        tmp_model.fit(X_train,y_train)\n","        tmp_train_feat = tmp_model.predict_proba(X_test)\n","        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n","        help_tfidf_train2[test_index] = tmp_train_feat\n","        help_tfidf_test2 += tmp_test_feat/feat_cnt\n","\n","        # count vec to nb\n","        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n","        tmp_model = MultinomialNB(0.025,fit_prior=False)\n","        tmp_model.fit(X_train,y_train)\n","        tmp_train_feat = tmp_model.predict_proba(X_test)\n","        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n","        help_cnt1_train[test_index] = tmp_train_feat\n","        help_cnt1_test += tmp_test_feat/feat_cnt\n","\n","        # count vec2 to nb \n","        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n","        tmp_model = MultinomialNB(0.025,fit_prior=False)\n","        tmp_model.fit(X_train,y_train)\n","        tmp_train_feat = tmp_model.predict_proba(X_test)\n","        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n","        help_cnt2_train[test_index] = tmp_train_feat\n","        help_cnt2_test += tmp_test_feat/feat_cnt\n","    \n","    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n","    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n","\n","    return help_train_feat,help_test_feat\n","    \n","help_train_feat,help_test_feat = gen_nb_feats(1)\n","help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n","help_train_feat3,help_test_feat3 = gen_nb_feats(3)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(54879, 2137725) (19617, 2137725)\n","(54879, 30) (19617, 30)\n","(54879, 2485843) (19617, 2485843)\n","(54879, 30) (19617, 30)\n","(54879, 2137725) (19617, 2137725)\n","(54879, 2485843) (19617, 2485843)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RxpqrgZFPbHo","executionInfo":{"status":"ok","timestamp":1607332035423,"user_tz":-540,"elapsed":1975420,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# libraries for Deep Learning\n","from keras.layers import Embedding, GRU, Dense, Flatten, Dropout\n","from keras.models import Sequential, load_model\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from sklearn import preprocessing\n","from sklearn.metrics import log_loss\n","import gc"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WZ40cNMLo3o"},"source":["# CNN을 이용한 feature 생성"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"x7p9fyELPbHp","executionInfo":{"status":"ok","timestamp":1607332035432,"user_tz":-540,"elapsed":1975423,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def get_cnn_feats(rnd=1):\n","    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    FEAT_CNT = 5\n","    NUM_WORDS = 30000\n","    N = 10\n","    MAX_LEN = 150\n","    NUM_CLASSES = 5\n","    MODEL_P = 'nn_model.h5'\n","    \n","    tmp_X = train_df['text']\n","    tmp_Y = train_df['author']\n","    tmp_X_test = test_df['text']\n","    \n","    tokenizer = Tokenizer(num_words=NUM_WORDS)\n","    tokenizer.fit_on_texts(tmp_X)\n","\n","    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n","    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n","    \n","    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n","    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n","\n","    lb = preprocessing.LabelBinarizer()\n","    lb.fit(tmp_Y)\n","\n","    ttrain_y = lb.transform(tmp_Y)\n","    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n","    for train_index, test_index in skf.split(train_tfidf,tmp_Y):\n","        model = Sequential()\n","        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n","        model.add(Conv1D(16,\n","                         3,\n","                         padding='valid',\n","                         activation='relu',\n","                         strides=1))\n","        model.add(GlobalAveragePooling1D())\n","        model.add(Dense(16, activation='relu'))\n","        model.add(Dropout(0.2))\n","        model.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n","        es = EarlyStopping(monitor='val_loss', patience=2)\n","\n","        np.random.seed(42)\n","        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n","                  validation_split=0.1,\n","                  batch_size=64, epochs=15, \n","                  verbose=1,\n","                  callbacks=[mc,es],\n","                  shuffle=False\n","                 )\n"," \n","        # feature 생성 1\n","        train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        # feature 생성 2\n","        model = load_model(MODEL_P)\n","        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        del model\n","        gc.collect()\n","        print('------------------')\n","        \n","    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfYctslAJflH"},"source":["get_cnn_feats 함수에서 인자는 단순히 seed값 변경의 의미만을 가지기 때문에,\n","굳이 3번이나 반복해야하나 싶어 feature 생성을 한번만 하였더니 이 또한 정확도 하락에\n","기여하였습니다. 이해할순 없지만 이러한 앙상블 역시 정확도에 기여하는 것을 볼 수 있습니다."]},{"cell_type":"code","metadata":{"id":"vGgTwpd6Jk0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607332638538,"user_tz":-540,"elapsed":2578508,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"ecd705ca-6c5d-4cb5-9307-e046302289c7"},"source":["cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n","cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n","cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","618/618 [==============================] - ETA: 0s - loss: 1.4870 - accuracy: 0.3495\n","Epoch 00001: val_loss improved from inf to 1.30919, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4870 - accuracy: 0.3495 - val_loss: 1.3092 - val_accuracy: 0.4543\n","Epoch 2/15\n","616/618 [============================>.] - ETA: 0s - loss: 1.1857 - accuracy: 0.5100\n","Epoch 00002: val_loss improved from 1.30919 to 1.08020, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1855 - accuracy: 0.5101 - val_loss: 1.0802 - val_accuracy: 0.5707\n","Epoch 3/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.9675 - accuracy: 0.6105\n","Epoch 00003: val_loss improved from 1.08020 to 0.94385, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9668 - accuracy: 0.6108 - val_loss: 0.9439 - val_accuracy: 0.6327\n","Epoch 4/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.8335 - accuracy: 0.6798\n","Epoch 00004: val_loss improved from 0.94385 to 0.89431, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.8327 - accuracy: 0.6805 - val_loss: 0.8943 - val_accuracy: 0.6570\n","Epoch 5/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.7386 - accuracy: 0.7224\n","Epoch 00005: val_loss improved from 0.89431 to 0.87432, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7386 - accuracy: 0.7225 - val_loss: 0.8743 - val_accuracy: 0.6709\n","Epoch 6/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.7525\n","Epoch 00006: val_loss improved from 0.87432 to 0.87427, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.6701 - accuracy: 0.7528 - val_loss: 0.8743 - val_accuracy: 0.6764\n","Epoch 7/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7745\n","Epoch 00007: val_loss did not improve from 0.87427\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6158 - accuracy: 0.7748 - val_loss: 0.8837 - val_accuracy: 0.6846\n","Epoch 8/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.5722 - accuracy: 0.7935\n","Epoch 00008: val_loss did not improve from 0.87427\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5719 - accuracy: 0.7937 - val_loss: 0.8991 - val_accuracy: 0.6850\n","------------------\n","Epoch 1/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.4933 - accuracy: 0.3425\n","Epoch 00001: val_loss improved from inf to 1.27793, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.4931 - accuracy: 0.3427 - val_loss: 1.2779 - val_accuracy: 0.5099\n","Epoch 2/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.1327 - accuracy: 0.5469\n","Epoch 00002: val_loss improved from 1.27793 to 1.03873, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1326 - accuracy: 0.5469 - val_loss: 1.0387 - val_accuracy: 0.6026\n","Epoch 3/15\n","618/618 [==============================] - ETA: 0s - loss: 0.9599 - accuracy: 0.6242\n","Epoch 00003: val_loss improved from 1.03873 to 0.96008, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9599 - accuracy: 0.6242 - val_loss: 0.9601 - val_accuracy: 0.6320\n","Epoch 4/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.8565 - accuracy: 0.6671\n","Epoch 00004: val_loss improved from 0.96008 to 0.91656, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8563 - accuracy: 0.6673 - val_loss: 0.9166 - val_accuracy: 0.6488\n","Epoch 5/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.7795 - accuracy: 0.6945\n","Epoch 00005: val_loss improved from 0.91656 to 0.89042, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7791 - accuracy: 0.6947 - val_loss: 0.8904 - val_accuracy: 0.6552\n","Epoch 6/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.7204 - accuracy: 0.7167\n","Epoch 00006: val_loss improved from 0.89042 to 0.88105, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7201 - accuracy: 0.7170 - val_loss: 0.8811 - val_accuracy: 0.6611\n","Epoch 7/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.7380\n","Epoch 00007: val_loss improved from 0.88105 to 0.86380, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6646 - accuracy: 0.7380 - val_loss: 0.8638 - val_accuracy: 0.6771\n","Epoch 8/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6138 - accuracy: 0.7637\n","Epoch 00008: val_loss improved from 0.86380 to 0.86086, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6138 - accuracy: 0.7637 - val_loss: 0.8609 - val_accuracy: 0.6905\n","Epoch 9/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7855\n","Epoch 00009: val_loss improved from 0.86086 to 0.84984, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5630 - accuracy: 0.7855 - val_loss: 0.8498 - val_accuracy: 0.7021\n","Epoch 10/15\n","618/618 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.8122\n","Epoch 00010: val_loss did not improve from 0.84984\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5142 - accuracy: 0.8122 - val_loss: 0.8549 - val_accuracy: 0.7103\n","Epoch 11/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.4719 - accuracy: 0.8304\n","Epoch 00011: val_loss did not improve from 0.84984\n","618/618 [==============================] - 4s 6ms/step - loss: 0.4719 - accuracy: 0.8304 - val_loss: 0.8751 - val_accuracy: 0.7165\n","------------------\n","Epoch 1/15\n","618/618 [==============================] - ETA: 0s - loss: 1.4964 - accuracy: 0.3277\n","Epoch 00001: val_loss improved from inf to 1.32375, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4964 - accuracy: 0.3277 - val_loss: 1.3238 - val_accuracy: 0.4425\n","Epoch 2/15\n","615/618 [============================>.] - ETA: 0s - loss: 1.1858 - accuracy: 0.4987\n","Epoch 00002: val_loss improved from 1.32375 to 1.11060, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1853 - accuracy: 0.4989 - val_loss: 1.1106 - val_accuracy: 0.5281\n","Epoch 3/15\n","618/618 [==============================] - ETA: 0s - loss: 1.0174 - accuracy: 0.5718\n","Epoch 00003: val_loss improved from 1.11060 to 1.02691, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.0174 - accuracy: 0.5718 - val_loss: 1.0269 - val_accuracy: 0.5760\n","Epoch 4/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.9070 - accuracy: 0.6338\n","Epoch 00004: val_loss improved from 1.02691 to 0.95175, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9069 - accuracy: 0.6338 - val_loss: 0.9518 - val_accuracy: 0.6256\n","Epoch 5/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.7937 - accuracy: 0.6878\n","Epoch 00005: val_loss improved from 0.95175 to 0.88912, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7936 - accuracy: 0.6877 - val_loss: 0.8891 - val_accuracy: 0.6529\n","Epoch 6/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.7017 - accuracy: 0.7334\n","Epoch 00006: val_loss improved from 0.88912 to 0.85179, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7013 - accuracy: 0.7336 - val_loss: 0.8518 - val_accuracy: 0.6789\n","Epoch 7/15\n","614/618 [============================>.] - ETA: 0s - loss: 0.6273 - accuracy: 0.7647\n","Epoch 00007: val_loss improved from 0.85179 to 0.83614, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6270 - accuracy: 0.7649 - val_loss: 0.8361 - val_accuracy: 0.6937\n","Epoch 8/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7915\n","Epoch 00008: val_loss improved from 0.83614 to 0.83312, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5654 - accuracy: 0.7916 - val_loss: 0.8331 - val_accuracy: 0.7026\n","Epoch 9/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.5131 - accuracy: 0.8158\n","Epoch 00009: val_loss did not improve from 0.83312\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5130 - accuracy: 0.8158 - val_loss: 0.8374 - val_accuracy: 0.7151\n","Epoch 10/15\n","618/618 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.8322\n","Epoch 00010: val_loss did not improve from 0.83312\n","618/618 [==============================] - 4s 6ms/step - loss: 0.4701 - accuracy: 0.8322 - val_loss: 0.8497 - val_accuracy: 0.7178\n","------------------\n","Epoch 1/15\n","614/618 [============================>.] - ETA: 0s - loss: 1.4789 - accuracy: 0.3538\n","Epoch 00001: val_loss improved from inf to 1.23820, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4773 - accuracy: 0.3548 - val_loss: 1.2382 - val_accuracy: 0.4917\n","Epoch 2/15\n","616/618 [============================>.] - ETA: 0s - loss: 1.0900 - accuracy: 0.5601\n","Epoch 00002: val_loss improved from 1.23820 to 1.02049, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.0896 - accuracy: 0.5604 - val_loss: 1.0205 - val_accuracy: 0.5851\n","Epoch 3/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.9208 - accuracy: 0.6312\n","Epoch 00003: val_loss improved from 1.02049 to 0.96566, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9201 - accuracy: 0.6318 - val_loss: 0.9657 - val_accuracy: 0.6078\n","Epoch 4/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.8208 - accuracy: 0.6759\n","Epoch 00004: val_loss improved from 0.96566 to 0.94951, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8200 - accuracy: 0.6764 - val_loss: 0.9495 - val_accuracy: 0.6247\n","Epoch 5/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.7440 - accuracy: 0.7109\n","Epoch 00005: val_loss improved from 0.94951 to 0.93576, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7434 - accuracy: 0.7112 - val_loss: 0.9358 - val_accuracy: 0.6322\n","Epoch 6/15\n","614/618 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.7386\n","Epoch 00006: val_loss did not improve from 0.93576\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6817 - accuracy: 0.7389 - val_loss: 0.9405 - val_accuracy: 0.6440\n","Epoch 7/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.6305 - accuracy: 0.7640\n","Epoch 00007: val_loss did not improve from 0.93576\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6305 - accuracy: 0.7640 - val_loss: 0.9683 - val_accuracy: 0.6479\n","------------------\n","Epoch 1/15\n","618/618 [==============================] - ETA: 0s - loss: 1.5060 - accuracy: 0.3157\n","Epoch 00001: val_loss improved from inf to 1.34971, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.5060 - accuracy: 0.3157 - val_loss: 1.3497 - val_accuracy: 0.4056\n","Epoch 2/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.1818 - accuracy: 0.4983\n","Epoch 00002: val_loss improved from 1.34971 to 1.11395, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1818 - accuracy: 0.4982 - val_loss: 1.1140 - val_accuracy: 0.5240\n","Epoch 3/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.0206 - accuracy: 0.5667\n","Epoch 00003: val_loss improved from 1.11395 to 1.04619, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.0201 - accuracy: 0.5672 - val_loss: 1.0462 - val_accuracy: 0.5550\n","Epoch 4/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.9396 - accuracy: 0.5980\n","Epoch 00004: val_loss improved from 1.04619 to 1.01091, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9392 - accuracy: 0.5984 - val_loss: 1.0109 - val_accuracy: 0.5775\n","Epoch 5/15\n","618/618 [==============================] - ETA: 0s - loss: 0.8756 - accuracy: 0.6383\n","Epoch 00005: val_loss improved from 1.01091 to 0.99098, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8756 - accuracy: 0.6383 - val_loss: 0.9910 - val_accuracy: 0.6037\n","Epoch 6/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.8196 - accuracy: 0.6709\n","Epoch 00006: val_loss improved from 0.99098 to 0.97445, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8189 - accuracy: 0.6713 - val_loss: 0.9745 - val_accuracy: 0.6235\n","Epoch 7/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.7624 - accuracy: 0.6974\n","Epoch 00007: val_loss improved from 0.97445 to 0.95785, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7624 - accuracy: 0.6974 - val_loss: 0.9578 - val_accuracy: 0.6395\n","Epoch 8/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.7181 - accuracy: 0.7143\n","Epoch 00008: val_loss did not improve from 0.95785\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7182 - accuracy: 0.7142 - val_loss: 0.9591 - val_accuracy: 0.6388\n","Epoch 9/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.7314\n","Epoch 00009: val_loss improved from 0.95785 to 0.95493, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.6751 - accuracy: 0.7315 - val_loss: 0.9549 - val_accuracy: 0.6509\n","Epoch 10/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.6358 - accuracy: 0.7516\n","Epoch 00010: val_loss did not improve from 0.95493\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6359 - accuracy: 0.7516 - val_loss: 0.9609 - val_accuracy: 0.6614\n","Epoch 11/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7684\n","Epoch 00011: val_loss did not improve from 0.95493\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5991 - accuracy: 0.7686 - val_loss: 0.9667 - val_accuracy: 0.6661\n","------------------\n","Epoch 1/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.5133 - accuracy: 0.3046\n","Epoch 00001: val_loss improved from inf to 1.41425, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.5132 - accuracy: 0.3048 - val_loss: 1.4143 - val_accuracy: 0.3366\n","Epoch 2/15\n","610/618 [============================>.] - ETA: 0s - loss: 1.2679 - accuracy: 0.4759\n","Epoch 00002: val_loss improved from 1.41425 to 1.07985, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.2657 - accuracy: 0.4773 - val_loss: 1.0799 - val_accuracy: 0.5862\n","Epoch 3/15\n","618/618 [==============================] - ETA: 0s - loss: 0.9638 - accuracy: 0.6264\n","Epoch 00003: val_loss improved from 1.07985 to 0.90680, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9638 - accuracy: 0.6264 - val_loss: 0.9068 - val_accuracy: 0.6502\n","Epoch 4/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.8090 - accuracy: 0.6952\n","Epoch 00004: val_loss improved from 0.90680 to 0.85009, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8086 - accuracy: 0.6953 - val_loss: 0.8501 - val_accuracy: 0.6737\n","Epoch 5/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.7105 - accuracy: 0.7388\n","Epoch 00005: val_loss improved from 0.85009 to 0.83479, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7104 - accuracy: 0.7388 - val_loss: 0.8348 - val_accuracy: 0.6837\n","Epoch 6/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6329 - accuracy: 0.7711\n","Epoch 00006: val_loss improved from 0.83479 to 0.83203, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6329 - accuracy: 0.7713 - val_loss: 0.8320 - val_accuracy: 0.6926\n","Epoch 7/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7967\n","Epoch 00007: val_loss did not improve from 0.83203\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5773 - accuracy: 0.7968 - val_loss: 0.8464 - val_accuracy: 0.6937\n","Epoch 8/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.5304 - accuracy: 0.8151\n","Epoch 00008: val_loss did not improve from 0.83203\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5302 - accuracy: 0.8152 - val_loss: 0.8720 - val_accuracy: 0.6944\n","------------------\n","Epoch 1/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.5250 - accuracy: 0.3123\n","Epoch 00001: val_loss improved from inf to 1.37436, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.5232 - accuracy: 0.3140 - val_loss: 1.3744 - val_accuracy: 0.4498\n","Epoch 2/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.1788 - accuracy: 0.5310\n","Epoch 00002: val_loss improved from 1.37436 to 1.05320, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1772 - accuracy: 0.5318 - val_loss: 1.0532 - val_accuracy: 0.5798\n","Epoch 3/15\n","614/618 [============================>.] - ETA: 0s - loss: 0.9759 - accuracy: 0.6145\n","Epoch 00003: val_loss improved from 1.05320 to 0.98330, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9755 - accuracy: 0.6148 - val_loss: 0.9833 - val_accuracy: 0.6056\n","Epoch 4/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.8745 - accuracy: 0.6603\n","Epoch 00004: val_loss improved from 0.98330 to 0.95786, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8744 - accuracy: 0.6603 - val_loss: 0.9579 - val_accuracy: 0.6181\n","Epoch 5/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.8026 - accuracy: 0.6931\n","Epoch 00005: val_loss improved from 0.95786 to 0.95484, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8026 - accuracy: 0.6932 - val_loss: 0.9548 - val_accuracy: 0.6286\n","Epoch 6/15\n","618/618 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.7176\n","Epoch 00006: val_loss did not improve from 0.95484\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7397 - accuracy: 0.7176 - val_loss: 0.9648 - val_accuracy: 0.6329\n","Epoch 7/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.7402\n","Epoch 00007: val_loss did not improve from 0.95484\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6908 - accuracy: 0.7404 - val_loss: 0.9807 - val_accuracy: 0.6379\n","------------------\n","Epoch 1/15\n","615/618 [============================>.] - ETA: 0s - loss: 1.4871 - accuracy: 0.3534\n","Epoch 00001: val_loss improved from inf to 1.26456, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4860 - accuracy: 0.3539 - val_loss: 1.2646 - val_accuracy: 0.4785\n","Epoch 2/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.1421 - accuracy: 0.5353\n","Epoch 00002: val_loss improved from 1.26456 to 1.06410, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1407 - accuracy: 0.5363 - val_loss: 1.0641 - val_accuracy: 0.5732\n","Epoch 3/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.9786 - accuracy: 0.6139\n","Epoch 00003: val_loss improved from 1.06410 to 0.98738, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9779 - accuracy: 0.6146 - val_loss: 0.9874 - val_accuracy: 0.6110\n","Epoch 4/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.8693 - accuracy: 0.6660\n","Epoch 00004: val_loss improved from 0.98738 to 0.94336, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8685 - accuracy: 0.6666 - val_loss: 0.9434 - val_accuracy: 0.6327\n","Epoch 5/15\n","618/618 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.7035\n","Epoch 00005: val_loss improved from 0.94336 to 0.92246, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7833 - accuracy: 0.7035 - val_loss: 0.9225 - val_accuracy: 0.6438\n","Epoch 6/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.7104 - accuracy: 0.7357\n","Epoch 00006: val_loss improved from 0.92246 to 0.91198, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7097 - accuracy: 0.7362 - val_loss: 0.9120 - val_accuracy: 0.6550\n","Epoch 7/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.7620\n","Epoch 00007: val_loss improved from 0.91198 to 0.90743, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6490 - accuracy: 0.7623 - val_loss: 0.9074 - val_accuracy: 0.6682\n","Epoch 8/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.7850\n","Epoch 00008: val_loss improved from 0.90743 to 0.90590, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5948 - accuracy: 0.7852 - val_loss: 0.9059 - val_accuracy: 0.6798\n","Epoch 9/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.8054\n","Epoch 00009: val_loss did not improve from 0.90590\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5455 - accuracy: 0.8056 - val_loss: 0.9081 - val_accuracy: 0.6873\n","Epoch 10/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.5056 - accuracy: 0.8228\n","Epoch 00010: val_loss did not improve from 0.90590\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5053 - accuracy: 0.8229 - val_loss: 0.9091 - val_accuracy: 0.6955\n","------------------\n","Epoch 1/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.5124 - accuracy: 0.3172\n","Epoch 00001: val_loss improved from inf to 1.36316, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.5108 - accuracy: 0.3185 - val_loss: 1.3632 - val_accuracy: 0.4284\n","Epoch 2/15\n","612/618 [============================>.] - ETA: 0s - loss: 1.2137 - accuracy: 0.4991\n","Epoch 00002: val_loss improved from 1.36316 to 1.10782, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.2123 - accuracy: 0.4999 - val_loss: 1.1078 - val_accuracy: 0.5454\n","Epoch 3/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.9963 - accuracy: 0.6086\n","Epoch 00003: val_loss improved from 1.10782 to 0.99524, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9956 - accuracy: 0.6090 - val_loss: 0.9952 - val_accuracy: 0.6053\n","Epoch 4/15\n","618/618 [==============================] - ETA: 0s - loss: 0.8785 - accuracy: 0.6604\n","Epoch 00004: val_loss improved from 0.99524 to 0.94027, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.8785 - accuracy: 0.6604 - val_loss: 0.9403 - val_accuracy: 0.6315\n","Epoch 5/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.7941 - accuracy: 0.6976\n","Epoch 00005: val_loss improved from 0.94027 to 0.91507, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7937 - accuracy: 0.6979 - val_loss: 0.9151 - val_accuracy: 0.6468\n","Epoch 6/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.7192 - accuracy: 0.7294\n","Epoch 00006: val_loss improved from 0.91507 to 0.90341, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7186 - accuracy: 0.7299 - val_loss: 0.9034 - val_accuracy: 0.6570\n","Epoch 7/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.7595\n","Epoch 00007: val_loss improved from 0.90341 to 0.89634, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6576 - accuracy: 0.7597 - val_loss: 0.8963 - val_accuracy: 0.6636\n","Epoch 8/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.6078 - accuracy: 0.7779\n","Epoch 00008: val_loss improved from 0.89634 to 0.89316, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6073 - accuracy: 0.7784 - val_loss: 0.8932 - val_accuracy: 0.6686\n","Epoch 9/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.5674 - accuracy: 0.7986\n","Epoch 00009: val_loss did not improve from 0.89316\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5674 - accuracy: 0.7985 - val_loss: 0.9122 - val_accuracy: 0.6739\n","Epoch 10/15\n","614/618 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.8122\n","Epoch 00010: val_loss did not improve from 0.89316\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5293 - accuracy: 0.8122 - val_loss: 0.9292 - val_accuracy: 0.6768\n","------------------\n","Epoch 1/15\n","610/618 [============================>.] - ETA: 0s - loss: 1.5153 - accuracy: 0.3012\n","Epoch 00001: val_loss improved from inf to 1.36148, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.5135 - accuracy: 0.3021 - val_loss: 1.3615 - val_accuracy: 0.3710\n","Epoch 2/15\n","612/618 [============================>.] - ETA: 0s - loss: 1.2330 - accuracy: 0.4481\n","Epoch 00002: val_loss improved from 1.36148 to 1.13298, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.2319 - accuracy: 0.4487 - val_loss: 1.1330 - val_accuracy: 0.5124\n","Epoch 3/15\n","615/618 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.5577\n","Epoch 00003: val_loss improved from 1.13298 to 1.00312, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.0349 - accuracy: 0.5579 - val_loss: 1.0031 - val_accuracy: 0.6067\n","Epoch 4/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.8782 - accuracy: 0.6500\n","Epoch 00004: val_loss improved from 1.00312 to 0.89663, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8776 - accuracy: 0.6503 - val_loss: 0.8966 - val_accuracy: 0.6602\n","Epoch 5/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.7425 - accuracy: 0.7129\n","Epoch 00005: val_loss improved from 0.89663 to 0.83490, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7423 - accuracy: 0.7132 - val_loss: 0.8349 - val_accuracy: 0.6875\n","Epoch 6/15\n","614/618 [============================>.] - ETA: 0s - loss: 0.6477 - accuracy: 0.7554\n","Epoch 00006: val_loss improved from 0.83490 to 0.80660, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6472 - accuracy: 0.7556 - val_loss: 0.8066 - val_accuracy: 0.7051\n","Epoch 7/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7842\n","Epoch 00007: val_loss improved from 0.80660 to 0.80123, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5798 - accuracy: 0.7846 - val_loss: 0.8012 - val_accuracy: 0.7165\n","Epoch 8/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.8076\n","Epoch 00008: val_loss did not improve from 0.80123\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5272 - accuracy: 0.8078 - val_loss: 0.8021 - val_accuracy: 0.7228\n","Epoch 9/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.4816 - accuracy: 0.8286\n","Epoch 00009: val_loss did not improve from 0.80123\n","618/618 [==============================] - 4s 6ms/step - loss: 0.4815 - accuracy: 0.8286 - val_loss: 0.8194 - val_accuracy: 0.7215\n","------------------\n","Epoch 1/15\n","615/618 [============================>.] - ETA: 0s - loss: 1.5205 - accuracy: 0.3200\n","Epoch 00001: val_loss improved from inf to 1.37825, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.5199 - accuracy: 0.3203 - val_loss: 1.3783 - val_accuracy: 0.4197\n","Epoch 2/15\n","615/618 [============================>.] - ETA: 0s - loss: 1.2589 - accuracy: 0.4690\n","Epoch 00002: val_loss improved from 1.37825 to 1.16989, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.2587 - accuracy: 0.4691 - val_loss: 1.1699 - val_accuracy: 0.5117\n","Epoch 3/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.0610 - accuracy: 0.5664\n","Epoch 00003: val_loss improved from 1.16989 to 1.03080, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.0610 - accuracy: 0.5664 - val_loss: 1.0308 - val_accuracy: 0.5928\n","Epoch 4/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.6416\n","Epoch 00004: val_loss improved from 1.03080 to 0.94946, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9155 - accuracy: 0.6417 - val_loss: 0.9495 - val_accuracy: 0.6333\n","Epoch 5/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.8119 - accuracy: 0.6923\n","Epoch 00005: val_loss improved from 0.94946 to 0.91089, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8120 - accuracy: 0.6922 - val_loss: 0.9109 - val_accuracy: 0.6461\n","Epoch 6/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.7307 - accuracy: 0.7284\n","Epoch 00006: val_loss improved from 0.91089 to 0.89892, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7307 - accuracy: 0.7284 - val_loss: 0.8989 - val_accuracy: 0.6527\n","Epoch 7/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.7589\n","Epoch 00007: val_loss did not improve from 0.89892\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6634 - accuracy: 0.7589 - val_loss: 0.8998 - val_accuracy: 0.6629\n","Epoch 8/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.6103 - accuracy: 0.7811\n","Epoch 00008: val_loss did not improve from 0.89892\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6101 - accuracy: 0.7814 - val_loss: 0.9130 - val_accuracy: 0.6675\n","------------------\n","Epoch 1/15\n","612/618 [============================>.] - ETA: 0s - loss: 1.5036 - accuracy: 0.3176\n","Epoch 00001: val_loss improved from inf to 1.36883, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.5025 - accuracy: 0.3186 - val_loss: 1.3688 - val_accuracy: 0.4263\n","Epoch 2/15\n","613/618 [============================>.] - ETA: 0s - loss: 1.2012 - accuracy: 0.5126\n","Epoch 00002: val_loss improved from 1.36883 to 1.04440, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.2001 - accuracy: 0.5131 - val_loss: 1.0444 - val_accuracy: 0.5839\n","Epoch 3/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.9544 - accuracy: 0.6230\n","Epoch 00003: val_loss improved from 1.04440 to 0.93115, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.9535 - accuracy: 0.6236 - val_loss: 0.9311 - val_accuracy: 0.6317\n","Epoch 4/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.8281 - accuracy: 0.6790\n","Epoch 00004: val_loss improved from 0.93115 to 0.88239, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.8278 - accuracy: 0.6793 - val_loss: 0.8824 - val_accuracy: 0.6541\n","Epoch 5/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.7351 - accuracy: 0.7225\n","Epoch 00005: val_loss improved from 0.88239 to 0.86336, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.7345 - accuracy: 0.7227 - val_loss: 0.8634 - val_accuracy: 0.6689\n","Epoch 6/15\n","618/618 [==============================] - ETA: 0s - loss: 0.6643 - accuracy: 0.7562\n","Epoch 00006: val_loss improved from 0.86336 to 0.86327, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6643 - accuracy: 0.7562 - val_loss: 0.8633 - val_accuracy: 0.6764\n","Epoch 7/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.7827\n","Epoch 00007: val_loss did not improve from 0.86327\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6070 - accuracy: 0.7829 - val_loss: 0.8687 - val_accuracy: 0.6885\n","Epoch 8/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.8020\n","Epoch 00008: val_loss did not improve from 0.86327\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5549 - accuracy: 0.8022 - val_loss: 0.8885 - val_accuracy: 0.6912\n","------------------\n","Epoch 1/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.4835 - accuracy: 0.3379\n","Epoch 00001: val_loss improved from inf to 1.29875, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4815 - accuracy: 0.3392 - val_loss: 1.2988 - val_accuracy: 0.4409\n","Epoch 2/15\n","611/618 [============================>.] - ETA: 0s - loss: 1.1335 - accuracy: 0.5304\n","Epoch 00002: val_loss improved from 1.29875 to 1.05485, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 1.1326 - accuracy: 0.5309 - val_loss: 1.0548 - val_accuracy: 0.5723\n","Epoch 3/15\n","613/618 [============================>.] - ETA: 0s - loss: 0.9501 - accuracy: 0.6166\n","Epoch 00003: val_loss improved from 1.05485 to 0.95286, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.9498 - accuracy: 0.6170 - val_loss: 0.9529 - val_accuracy: 0.6279\n","Epoch 4/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.8275 - accuracy: 0.6782\n","Epoch 00004: val_loss improved from 0.95286 to 0.89981, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.8274 - accuracy: 0.6782 - val_loss: 0.8998 - val_accuracy: 0.6568\n","Epoch 5/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.7256 - accuracy: 0.7235\n","Epoch 00005: val_loss improved from 0.89981 to 0.87627, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7258 - accuracy: 0.7236 - val_loss: 0.8763 - val_accuracy: 0.6741\n","Epoch 6/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.6533 - accuracy: 0.7588\n","Epoch 00006: val_loss improved from 0.87627 to 0.87623, saving model to nn_model.h5\n","618/618 [==============================] - 4s 6ms/step - loss: 0.6531 - accuracy: 0.7589 - val_loss: 0.8762 - val_accuracy: 0.6796\n","Epoch 7/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.5850 - accuracy: 0.7913\n","Epoch 00007: val_loss did not improve from 0.87623\n","618/618 [==============================] - 4s 7ms/step - loss: 0.5850 - accuracy: 0.7912 - val_loss: 0.8941 - val_accuracy: 0.6775\n","Epoch 8/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.8125\n","Epoch 00008: val_loss did not improve from 0.87623\n","618/618 [==============================] - 4s 6ms/step - loss: 0.5359 - accuracy: 0.8124 - val_loss: 0.9163 - val_accuracy: 0.6828\n","------------------\n","Epoch 1/15\n","617/618 [============================>.] - ETA: 0s - loss: 1.5137 - accuracy: 0.3116\n","Epoch 00001: val_loss improved from inf to 1.39196, saving model to nn_model.h5\n","618/618 [==============================] - 5s 7ms/step - loss: 1.5135 - accuracy: 0.3117 - val_loss: 1.3920 - val_accuracy: 0.3717\n","Epoch 2/15\n","610/618 [============================>.] - ETA: 0s - loss: 1.1890 - accuracy: 0.5277\n","Epoch 00002: val_loss improved from 1.39196 to 1.03412, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.1873 - accuracy: 0.5288 - val_loss: 1.0341 - val_accuracy: 0.6053\n","Epoch 3/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.9394 - accuracy: 0.6407\n","Epoch 00003: val_loss improved from 1.03412 to 0.91524, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.9392 - accuracy: 0.6407 - val_loss: 0.9152 - val_accuracy: 0.6452\n","Epoch 4/15\n","616/618 [============================>.] - ETA: 0s - loss: 0.8073 - accuracy: 0.6940\n","Epoch 00004: val_loss improved from 0.91524 to 0.86601, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.8074 - accuracy: 0.6941 - val_loss: 0.8660 - val_accuracy: 0.6627\n","Epoch 5/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.7131 - accuracy: 0.7374\n","Epoch 00005: val_loss improved from 0.86601 to 0.84406, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7131 - accuracy: 0.7379 - val_loss: 0.8441 - val_accuracy: 0.6828\n","Epoch 6/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.6348 - accuracy: 0.7687\n","Epoch 00006: val_loss improved from 0.84406 to 0.83944, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.6348 - accuracy: 0.7690 - val_loss: 0.8394 - val_accuracy: 0.6926\n","Epoch 7/15\n","610/618 [============================>.] - ETA: 0s - loss: 0.5766 - accuracy: 0.7966\n","Epoch 00007: val_loss did not improve from 0.83944\n","618/618 [==============================] - 4s 7ms/step - loss: 0.5767 - accuracy: 0.7967 - val_loss: 0.8477 - val_accuracy: 0.6932\n","Epoch 8/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.8139\n","Epoch 00008: val_loss did not improve from 0.83944\n","618/618 [==============================] - 4s 7ms/step - loss: 0.5262 - accuracy: 0.8140 - val_loss: 0.8655 - val_accuracy: 0.6937\n","------------------\n","Epoch 1/15\n","614/618 [============================>.] - ETA: 0s - loss: 1.4716 - accuracy: 0.3616\n","Epoch 00001: val_loss improved from inf to 1.23530, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.4701 - accuracy: 0.3626 - val_loss: 1.2353 - val_accuracy: 0.4969\n","Epoch 2/15\n","616/618 [============================>.] - ETA: 0s - loss: 1.0849 - accuracy: 0.5761\n","Epoch 00002: val_loss improved from 1.23530 to 0.99352, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 1.0846 - accuracy: 0.5763 - val_loss: 0.9935 - val_accuracy: 0.6126\n","Epoch 3/15\n","617/618 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.6652\n","Epoch 00003: val_loss improved from 0.99352 to 0.89174, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.8818 - accuracy: 0.6652 - val_loss: 0.8917 - val_accuracy: 0.6582\n","Epoch 4/15\n","612/618 [============================>.] - ETA: 0s - loss: 0.7550 - accuracy: 0.7201\n","Epoch 00004: val_loss improved from 0.89174 to 0.83344, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.7546 - accuracy: 0.7201 - val_loss: 0.8334 - val_accuracy: 0.6818\n","Epoch 5/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.6622 - accuracy: 0.7585\n","Epoch 00005: val_loss improved from 0.83344 to 0.80356, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.6613 - accuracy: 0.7588 - val_loss: 0.8036 - val_accuracy: 0.7019\n","Epoch 6/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.7884\n","Epoch 00006: val_loss improved from 0.80356 to 0.78997, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.5894 - accuracy: 0.7886 - val_loss: 0.7900 - val_accuracy: 0.7142\n","Epoch 7/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.5331 - accuracy: 0.8113\n","Epoch 00007: val_loss improved from 0.78997 to 0.78476, saving model to nn_model.h5\n","618/618 [==============================] - 4s 7ms/step - loss: 0.5327 - accuracy: 0.8116 - val_loss: 0.7848 - val_accuracy: 0.7222\n","Epoch 8/15\n","615/618 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.8281\n","Epoch 00008: val_loss did not improve from 0.78476\n","618/618 [==============================] - 4s 7ms/step - loss: 0.4844 - accuracy: 0.8282 - val_loss: 0.8053 - val_accuracy: 0.7247\n","Epoch 9/15\n","611/618 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.8438\n","Epoch 00009: val_loss did not improve from 0.78476\n","618/618 [==============================] - 4s 7ms/step - loss: 0.4487 - accuracy: 0.8441 - val_loss: 0.8315 - val_accuracy: 0.7249\n","------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XVJyw-eLLrB9"},"source":["# GRU를 이용한 feature 생성"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"X3IH9cr8PbHp","executionInfo":{"status":"ok","timestamp":1607332638540,"user_tz":-540,"elapsed":2578506,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def get_gru_feats(rnd=1):\n","    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    FEAT_CNT = 5\n","    NUM_WORDS = 16000\n","    N = 12\n","    MAX_LEN = 300\n","    NUM_CLASSES = 5\n","    MODEL_P = 'nn_model.h5'\n","    \n","    tmp_X = train_df['text']\n","    tmp_Y = train_df['author']\n","    tmp_X_test = test_df['text']\n","    \n","    tokenizer = Tokenizer(num_words=NUM_WORDS)\n","    tokenizer.fit_on_texts(tmp_X)\n","\n","    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n","    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n","    \n","    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n","    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n","\n","    lb = preprocessing.LabelBinarizer()\n","    lb.fit(tmp_Y)\n","\n","    ttrain_y = lb.transform(tmp_Y)\n","    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n","    for train_index, test_index in skf.split(ttrain_x,tmp_Y):\n","        model = Sequential()\n","        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n","        model.add(GRU(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n","        model.add(Flatten())\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(0.2))\n","        model.add(Dense(NUM_CLASSES, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n","        es=EarlyStopping(monitor='val_loss', patience=2)\n","\n","        np.random.seed(42)\n","        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n","                  validation_split=0.1,\n","                  batch_size=256, epochs=10, \n","                  verbose=1,\n","                  callbacks=[mc,es],\n","                  shuffle=False\n","                 )\n","        \n","        # feature 생성 1\n","        train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        # feature 생성 2\n","        model = load_model(MODEL_P)\n","        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        del model\n","        gc.collect()\n","        print('------------------')\n","        \n","    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uC_usDwrJ3vZ"},"source":["뒤늦게 안 사실인데 해당 데이터에서는 GRU보다 LSTM이 마지막 앙상블에서 더 성능이 좋습니다."]},{"cell_type":"code","metadata":{"id":"ToCxtAyIJ4Ol","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607338907346,"user_tz":-540,"elapsed":8847298,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"ea54d60c-e716-4d4a-b447-9f55ae65ab97"},"source":["gru_train1,gru_test1,gru_train2,gru_test2 = get_gru_feats(1)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Epoch 1/10\n","155/155 [==============================] - ETA: 0s - loss: 1.3735 - accuracy: 0.4142\n","Epoch 00001: val_loss improved from inf to 1.00951, saving model to nn_model.h5\n","155/155 [==============================] - 149s 964ms/step - loss: 1.3735 - accuracy: 0.4142 - val_loss: 1.0095 - val_accuracy: 0.6010\n","Epoch 2/10\n","155/155 [==============================] - ETA: 0s - loss: 0.8488 - accuracy: 0.6764\n","Epoch 00002: val_loss improved from 1.00951 to 0.73846, saving model to nn_model.h5\n","155/155 [==============================] - 150s 968ms/step - loss: 0.8488 - accuracy: 0.6764 - val_loss: 0.7385 - val_accuracy: 0.7194\n","Epoch 3/10\n","155/155 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.7706\n","Epoch 00003: val_loss improved from 0.73846 to 0.68376, saving model to nn_model.h5\n","155/155 [==============================] - 152s 982ms/step - loss: 0.6246 - accuracy: 0.7706 - val_loss: 0.6838 - val_accuracy: 0.7420\n","Epoch 4/10\n","155/155 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.8180\n","Epoch 00004: val_loss improved from 0.68376 to 0.66434, saving model to nn_model.h5\n","155/155 [==============================] - 152s 981ms/step - loss: 0.5006 - accuracy: 0.8180 - val_loss: 0.6643 - val_accuracy: 0.7577\n","Epoch 5/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8474\n","Epoch 00005: val_loss improved from 0.66434 to 0.65235, saving model to nn_model.h5\n","155/155 [==============================] - 154s 995ms/step - loss: 0.4241 - accuracy: 0.8474 - val_loss: 0.6523 - val_accuracy: 0.7645\n","Epoch 6/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8667\n","Epoch 00006: val_loss did not improve from 0.65235\n","155/155 [==============================] - 162s 1s/step - loss: 0.3738 - accuracy: 0.8667 - val_loss: 0.6948 - val_accuracy: 0.7602\n","Epoch 7/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.8788\n","Epoch 00007: val_loss did not improve from 0.65235\n","155/155 [==============================] - 158s 1s/step - loss: 0.3403 - accuracy: 0.8788 - val_loss: 0.7342 - val_accuracy: 0.7584\n","WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","------------------\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Epoch 1/10\n","155/155 [==============================] - ETA: 0s - loss: 1.3193 - accuracy: 0.4461\n","Epoch 00001: val_loss improved from inf to 0.99421, saving model to nn_model.h5\n","155/155 [==============================] - 153s 987ms/step - loss: 1.3193 - accuracy: 0.4461 - val_loss: 0.9942 - val_accuracy: 0.6169\n","Epoch 2/10\n","155/155 [==============================] - ETA: 0s - loss: 0.8386 - accuracy: 0.6786\n","Epoch 00002: val_loss improved from 0.99421 to 0.77778, saving model to nn_model.h5\n","155/155 [==============================] - 155s 1s/step - loss: 0.8386 - accuracy: 0.6786 - val_loss: 0.7778 - val_accuracy: 0.6976\n","Epoch 3/10\n","155/155 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.7728\n","Epoch 00003: val_loss improved from 0.77778 to 0.70057, saving model to nn_model.h5\n","155/155 [==============================] - 163s 1s/step - loss: 0.6176 - accuracy: 0.7728 - val_loss: 0.7006 - val_accuracy: 0.7349\n","Epoch 4/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.8204\n","Epoch 00004: val_loss improved from 0.70057 to 0.68212, saving model to nn_model.h5\n","155/155 [==============================] - 164s 1s/step - loss: 0.4936 - accuracy: 0.8204 - val_loss: 0.6821 - val_accuracy: 0.7488\n","Epoch 5/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8486\n","Epoch 00005: val_loss did not improve from 0.68212\n","155/155 [==============================] - 164s 1s/step - loss: 0.4187 - accuracy: 0.8486 - val_loss: 0.6985 - val_accuracy: 0.7588\n","Epoch 6/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8674\n","Epoch 00006: val_loss did not improve from 0.68212\n","155/155 [==============================] - 164s 1s/step - loss: 0.3682 - accuracy: 0.8674 - val_loss: 0.7257 - val_accuracy: 0.7570\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","------------------\n","WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Epoch 1/10\n","155/155 [==============================] - ETA: 0s - loss: 1.3219 - accuracy: 0.4370\n","Epoch 00001: val_loss improved from inf to 0.99159, saving model to nn_model.h5\n","155/155 [==============================] - 166s 1s/step - loss: 1.3219 - accuracy: 0.4370 - val_loss: 0.9916 - val_accuracy: 0.6072\n","Epoch 2/10\n","155/155 [==============================] - ETA: 0s - loss: 0.8540 - accuracy: 0.6706\n","Epoch 00002: val_loss improved from 0.99159 to 0.82868, saving model to nn_model.h5\n","155/155 [==============================] - 166s 1s/step - loss: 0.8540 - accuracy: 0.6706 - val_loss: 0.8287 - val_accuracy: 0.6823\n","Epoch 3/10\n","155/155 [==============================] - ETA: 0s - loss: 0.6575 - accuracy: 0.7560\n","Epoch 00003: val_loss improved from 0.82868 to 0.70452, saving model to nn_model.h5\n","155/155 [==============================] - 166s 1s/step - loss: 0.6575 - accuracy: 0.7560 - val_loss: 0.7045 - val_accuracy: 0.7342\n","Epoch 4/10\n","155/155 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.8058\n","Epoch 00004: val_loss improved from 0.70452 to 0.69488, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 0.5408 - accuracy: 0.8058 - val_loss: 0.6949 - val_accuracy: 0.7431\n","Epoch 5/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.8294\n","Epoch 00005: val_loss improved from 0.69488 to 0.68921, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 0.4689 - accuracy: 0.8294 - val_loss: 0.6892 - val_accuracy: 0.7461\n","Epoch 6/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8507\n","Epoch 00006: val_loss did not improve from 0.68921\n","155/155 [==============================] - 164s 1s/step - loss: 0.4156 - accuracy: 0.8507 - val_loss: 0.6973 - val_accuracy: 0.7513\n","Epoch 7/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.8666\n","Epoch 00007: val_loss did not improve from 0.68921\n","155/155 [==============================] - 163s 1s/step - loss: 0.3751 - accuracy: 0.8666 - val_loss: 0.7191 - val_accuracy: 0.7552\n","WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","------------------\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Epoch 1/10\n","155/155 [==============================] - ETA: 0s - loss: 1.3258 - accuracy: 0.4363\n","Epoch 00001: val_loss improved from inf to 1.06818, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 1.3258 - accuracy: 0.4363 - val_loss: 1.0682 - val_accuracy: 0.5618\n","Epoch 2/10\n","155/155 [==============================] - ETA: 0s - loss: 0.9425 - accuracy: 0.6180\n","Epoch 00002: val_loss improved from 1.06818 to 0.87752, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 0.9425 - accuracy: 0.6180 - val_loss: 0.8775 - val_accuracy: 0.6618\n","Epoch 3/10\n","155/155 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.7221\n","Epoch 00003: val_loss improved from 0.87752 to 0.74876, saving model to nn_model.h5\n","155/155 [==============================] - 166s 1s/step - loss: 0.7297 - accuracy: 0.7221 - val_loss: 0.7488 - val_accuracy: 0.7199\n","Epoch 4/10\n","155/155 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.7864\n","Epoch 00004: val_loss improved from 0.74876 to 0.70917, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 0.5782 - accuracy: 0.7864 - val_loss: 0.7092 - val_accuracy: 0.7376\n","Epoch 5/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.8214\n","Epoch 00005: val_loss improved from 0.70917 to 0.69914, saving model to nn_model.h5\n","155/155 [==============================] - 165s 1s/step - loss: 0.4893 - accuracy: 0.8214 - val_loss: 0.6991 - val_accuracy: 0.7458\n","Epoch 6/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8435\n","Epoch 00006: val_loss did not improve from 0.69914\n","155/155 [==============================] - 165s 1s/step - loss: 0.4297 - accuracy: 0.8435 - val_loss: 0.7185 - val_accuracy: 0.7461\n","Epoch 7/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8587\n","Epoch 00007: val_loss did not improve from 0.69914\n","155/155 [==============================] - 165s 1s/step - loss: 0.3924 - accuracy: 0.8587 - val_loss: 0.7407 - val_accuracy: 0.7481\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","------------------\n","WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Epoch 1/10\n","155/155 [==============================] - ETA: 0s - loss: 1.3867 - accuracy: 0.4005\n","Epoch 00001: val_loss improved from inf to 1.07229, saving model to nn_model.h5\n","155/155 [==============================] - 167s 1s/step - loss: 1.3867 - accuracy: 0.4005 - val_loss: 1.0723 - val_accuracy: 0.5637\n","Epoch 2/10\n","155/155 [==============================] - ETA: 0s - loss: 0.8790 - accuracy: 0.6542\n","Epoch 00002: val_loss improved from 1.07229 to 0.80277, saving model to nn_model.h5\n","155/155 [==============================] - 167s 1s/step - loss: 0.8790 - accuracy: 0.6542 - val_loss: 0.8028 - val_accuracy: 0.6866\n","Epoch 3/10\n","155/155 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.7550\n","Epoch 00003: val_loss improved from 0.80277 to 0.71294, saving model to nn_model.h5\n","155/155 [==============================] - 167s 1s/step - loss: 0.6554 - accuracy: 0.7550 - val_loss: 0.7129 - val_accuracy: 0.7326\n","Epoch 4/10\n","155/155 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8098\n","Epoch 00004: val_loss improved from 0.71294 to 0.68183, saving model to nn_model.h5\n","155/155 [==============================] - 168s 1s/step - loss: 0.5258 - accuracy: 0.8098 - val_loss: 0.6818 - val_accuracy: 0.7520\n","Epoch 5/10\n","155/155 [==============================] - ETA: 0s - loss: 0.4434 - accuracy: 0.8393\n","Epoch 00005: val_loss did not improve from 0.68183\n","155/155 [==============================] - 167s 1s/step - loss: 0.4434 - accuracy: 0.8393 - val_loss: 0.6903 - val_accuracy: 0.7550\n","Epoch 6/10\n","155/155 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8609\n","Epoch 00006: val_loss did not improve from 0.68183\n","155/155 [==============================] - 166s 1s/step - loss: 0.3869 - accuracy: 0.8609 - val_loss: 0.7091 - val_accuracy: 0.7545\n","WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sT_eb8qdLwBc"},"source":["# NN을 이용한 feature 생성"]},{"cell_type":"code","metadata":{"id":"y994I1AzPbHq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607339429564,"user_tz":-540,"elapsed":9369502,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"ffe56135-419b-4cbc-b350-fea624fd15c2"},"source":["# NN은 (https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31)에서 최고의 정확도를 냅니다.\n","# 하지만 이 코드에 결합할 시 더 나쁜 결과를 내어 적용하진 않았습니다.\n","\n","def get_nn_feats(rnd=1):\n","    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n","    FEAT_CNT = 10\n","    NUM_WORDS = 30000\n","    N = 10\n","    MAX_LEN = 100\n","    NUM_CLASSES = 5\n","    MODEL_P = 'nn_model.h5'\n","    \n","    tmp_X = train_df['text']\n","    tmp_Y = train_df['author']\n","    tmp_X_test = test_df['text']\n","    \n","    tokenizer = Tokenizer(num_words=NUM_WORDS)\n","    tokenizer.fit_on_texts(tmp_X)\n","\n","    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n","    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n","    \n","    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n","    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n","\n","    lb = preprocessing.LabelBinarizer()\n","    lb.fit(tmp_Y)\n","\n","    ttrain_y = lb.transform(tmp_Y)\n","    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n","    for train_index, test_index in skf.split(ttrain_x,tmp_Y):\n","        model = Sequential()\n","        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n","        model.add(GlobalAveragePooling1D())\n","        model.add(Dense(30, activation='relu'))\n","        model.add(Dropout(0.1))\n","        model.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n","        es=EarlyStopping(monitor='val_loss', patience=2)\n","\n","        np.random.seed(42)\n","        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n","                  validation_split=0.3,\n","                  batch_size=64, epochs=20, \n","                  verbose=1,\n","                  callbacks=[mc,es],\n","                  shuffle=False\n","                 )\n"," \n","        # feature 생성 1\n","        train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        # feature 생성 2\n","        model = load_model(MODEL_P)\n","        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n","        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n","        \n","        del model\n","        gc.collect()\n","        print('------------------')\n","        \n","    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n","\n","nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(1)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","534/541 [============================>.] - ETA: 0s - loss: 1.5097 - accuracy: 0.3330\n","Epoch 00001: val_loss improved from inf to 1.36640, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.5084 - accuracy: 0.3339 - val_loss: 1.3664 - val_accuracy: 0.4383\n","Epoch 2/20\n","533/541 [============================>.] - ETA: 0s - loss: 1.2455 - accuracy: 0.4832\n","Epoch 00002: val_loss improved from 1.36640 to 1.15079, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2443 - accuracy: 0.4839 - val_loss: 1.1508 - val_accuracy: 0.5450\n","Epoch 3/20\n","538/541 [============================>.] - ETA: 0s - loss: 1.0427 - accuracy: 0.5853\n","Epoch 00003: val_loss improved from 1.15079 to 1.00556, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.0427 - accuracy: 0.5853 - val_loss: 1.0056 - val_accuracy: 0.6182\n","Epoch 4/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.8934 - accuracy: 0.6616\n","Epoch 00004: val_loss improved from 1.00556 to 0.90711, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8927 - accuracy: 0.6619 - val_loss: 0.9071 - val_accuracy: 0.6651\n","Epoch 5/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7145\n","Epoch 00005: val_loss improved from 0.90711 to 0.83678, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7756 - accuracy: 0.7149 - val_loss: 0.8368 - val_accuracy: 0.6979\n","Epoch 6/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.7578\n","Epoch 00006: val_loss improved from 0.83678 to 0.79010, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6774 - accuracy: 0.7579 - val_loss: 0.7901 - val_accuracy: 0.7151\n","Epoch 7/20\n","541/541 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7906\n","Epoch 00007: val_loss improved from 0.79010 to 0.75538, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5980 - accuracy: 0.7906 - val_loss: 0.7554 - val_accuracy: 0.7299\n","Epoch 8/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.5334 - accuracy: 0.8129\n","Epoch 00008: val_loss improved from 0.75538 to 0.73594, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5335 - accuracy: 0.8130 - val_loss: 0.7359 - val_accuracy: 0.7375\n","Epoch 9/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.8343\n","Epoch 00009: val_loss improved from 0.73594 to 0.72527, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4791 - accuracy: 0.8344 - val_loss: 0.7253 - val_accuracy: 0.7442\n","Epoch 10/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8523\n","Epoch 00010: val_loss improved from 0.72527 to 0.72456, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4345 - accuracy: 0.8523 - val_loss: 0.7246 - val_accuracy: 0.7465\n","Epoch 11/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8653\n","Epoch 00011: val_loss improved from 0.72456 to 0.71962, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3971 - accuracy: 0.8653 - val_loss: 0.7196 - val_accuracy: 0.7512\n","Epoch 12/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8783\n","Epoch 00012: val_loss did not improve from 0.71962\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3631 - accuracy: 0.8785 - val_loss: 0.7278 - val_accuracy: 0.7518\n","Epoch 13/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.8879\n","Epoch 00013: val_loss did not improve from 0.71962\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3340 - accuracy: 0.8880 - val_loss: 0.7382 - val_accuracy: 0.7543\n","------------------\n","Epoch 1/20\n","534/541 [============================>.] - ETA: 0s - loss: 1.5160 - accuracy: 0.3482\n","Epoch 00001: val_loss improved from inf to 1.36797, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5142 - accuracy: 0.3499 - val_loss: 1.3680 - val_accuracy: 0.4611\n","Epoch 2/20\n","536/541 [============================>.] - ETA: 0s - loss: 1.1753 - accuracy: 0.5502\n","Epoch 00002: val_loss improved from 1.36797 to 1.03191, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.1739 - accuracy: 0.5506 - val_loss: 1.0319 - val_accuracy: 0.6068\n","Epoch 3/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.9275 - accuracy: 0.6460\n","Epoch 00003: val_loss improved from 1.03191 to 0.90139, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9272 - accuracy: 0.6461 - val_loss: 0.9014 - val_accuracy: 0.6561\n","Epoch 4/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.7998 - accuracy: 0.6995\n","Epoch 00004: val_loss improved from 0.90139 to 0.83046, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7997 - accuracy: 0.6995 - val_loss: 0.8305 - val_accuracy: 0.6852\n","Epoch 5/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.7080 - accuracy: 0.7391\n","Epoch 00005: val_loss improved from 0.83046 to 0.78171, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7073 - accuracy: 0.7393 - val_loss: 0.7817 - val_accuracy: 0.7085\n","Epoch 6/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.6288 - accuracy: 0.7735\n","Epoch 00006: val_loss improved from 0.78171 to 0.74758, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6286 - accuracy: 0.7736 - val_loss: 0.7476 - val_accuracy: 0.7242\n","Epoch 7/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7990\n","Epoch 00007: val_loss improved from 0.74758 to 0.72470, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5652 - accuracy: 0.7992 - val_loss: 0.7247 - val_accuracy: 0.7343\n","Epoch 8/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.5113 - accuracy: 0.8212\n","Epoch 00008: val_loss improved from 0.72470 to 0.71229, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5109 - accuracy: 0.8214 - val_loss: 0.7123 - val_accuracy: 0.7429\n","Epoch 9/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.8412\n","Epoch 00009: val_loss improved from 0.71229 to 0.70405, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4633 - accuracy: 0.8413 - val_loss: 0.7041 - val_accuracy: 0.7473\n","Epoch 10/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8547\n","Epoch 00010: val_loss improved from 0.70405 to 0.70074, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4229 - accuracy: 0.8550 - val_loss: 0.7007 - val_accuracy: 0.7507\n","Epoch 11/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8697\n","Epoch 00011: val_loss improved from 0.70074 to 0.69886, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3868 - accuracy: 0.8697 - val_loss: 0.6989 - val_accuracy: 0.7567\n","Epoch 12/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8809\n","Epoch 00012: val_loss did not improve from 0.69886\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3573 - accuracy: 0.8809 - val_loss: 0.7016 - val_accuracy: 0.7583\n","Epoch 13/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8904\n","Epoch 00013: val_loss did not improve from 0.69886\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3292 - accuracy: 0.8906 - val_loss: 0.7083 - val_accuracy: 0.7614\n","------------------\n","Epoch 1/20\n","535/541 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3186\n","Epoch 00001: val_loss improved from inf to 1.40603, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.5258 - accuracy: 0.3194 - val_loss: 1.4060 - val_accuracy: 0.4186\n","Epoch 2/20\n","536/541 [============================>.] - ETA: 0s - loss: 1.2401 - accuracy: 0.5021\n","Epoch 00002: val_loss improved from 1.40603 to 1.10220, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2389 - accuracy: 0.5025 - val_loss: 1.1022 - val_accuracy: 0.5759\n","Epoch 3/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.9873 - accuracy: 0.6181\n","Epoch 00003: val_loss improved from 1.10220 to 0.94033, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9874 - accuracy: 0.6181 - val_loss: 0.9403 - val_accuracy: 0.6364\n","Epoch 4/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.8383 - accuracy: 0.6795\n","Epoch 00004: val_loss improved from 0.94033 to 0.85662, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8384 - accuracy: 0.6795 - val_loss: 0.8566 - val_accuracy: 0.6765\n","Epoch 5/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.7371 - accuracy: 0.7240\n","Epoch 00005: val_loss improved from 0.85662 to 0.80265, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7371 - accuracy: 0.7241 - val_loss: 0.8027 - val_accuracy: 0.6995\n","Epoch 6/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.7612\n","Epoch 00006: val_loss improved from 0.80265 to 0.76701, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6527 - accuracy: 0.7614 - val_loss: 0.7670 - val_accuracy: 0.7149\n","Epoch 7/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.5822 - accuracy: 0.7903\n","Epoch 00007: val_loss improved from 0.76701 to 0.74278, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5820 - accuracy: 0.7904 - val_loss: 0.7428 - val_accuracy: 0.7256\n","Epoch 8/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.8156\n","Epoch 00008: val_loss improved from 0.74278 to 0.73074, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5232 - accuracy: 0.8157 - val_loss: 0.7307 - val_accuracy: 0.7319\n","Epoch 9/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.4733 - accuracy: 0.8342\n","Epoch 00009: val_loss improved from 0.73074 to 0.72215, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4730 - accuracy: 0.8345 - val_loss: 0.7221 - val_accuracy: 0.7393\n","Epoch 10/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.8503\n","Epoch 00010: val_loss did not improve from 0.72215\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4315 - accuracy: 0.8503 - val_loss: 0.7222 - val_accuracy: 0.7410\n","Epoch 11/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8653\n","Epoch 00011: val_loss did not improve from 0.72215\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3921 - accuracy: 0.8655 - val_loss: 0.7226 - val_accuracy: 0.7454\n","------------------\n","Epoch 1/20\n","535/541 [============================>.] - ETA: 0s - loss: 1.5372 - accuracy: 0.3314\n","Epoch 00001: val_loss improved from inf to 1.41988, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5359 - accuracy: 0.3328 - val_loss: 1.4199 - val_accuracy: 0.4665\n","Epoch 2/20\n","535/541 [============================>.] - ETA: 0s - loss: 1.1956 - accuracy: 0.5555\n","Epoch 00002: val_loss improved from 1.41988 to 1.03622, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.1939 - accuracy: 0.5559 - val_loss: 1.0362 - val_accuracy: 0.5989\n","Epoch 3/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.9329 - accuracy: 0.6451\n","Epoch 00003: val_loss improved from 1.03622 to 0.90375, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9328 - accuracy: 0.6452 - val_loss: 0.9038 - val_accuracy: 0.6495\n","Epoch 4/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.8043 - accuracy: 0.6950\n","Epoch 00004: val_loss improved from 0.90375 to 0.83096, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8043 - accuracy: 0.6949 - val_loss: 0.8310 - val_accuracy: 0.6839\n","Epoch 5/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.7373\n","Epoch 00005: val_loss improved from 0.83096 to 0.78261, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7097 - accuracy: 0.7373 - val_loss: 0.7826 - val_accuracy: 0.7043\n","Epoch 6/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.6341 - accuracy: 0.7678\n","Epoch 00006: val_loss improved from 0.78261 to 0.75066, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6342 - accuracy: 0.7679 - val_loss: 0.7507 - val_accuracy: 0.7185\n","Epoch 7/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7941\n","Epoch 00007: val_loss improved from 0.75066 to 0.72833, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5707 - accuracy: 0.7942 - val_loss: 0.7283 - val_accuracy: 0.7307\n","Epoch 8/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.8184\n","Epoch 00008: val_loss improved from 0.72833 to 0.71299, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5148 - accuracy: 0.8186 - val_loss: 0.7130 - val_accuracy: 0.7405\n","Epoch 9/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.8388\n","Epoch 00009: val_loss improved from 0.71299 to 0.70364, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4664 - accuracy: 0.8389 - val_loss: 0.7036 - val_accuracy: 0.7485\n","Epoch 10/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.8531\n","Epoch 00010: val_loss improved from 0.70364 to 0.69744, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4245 - accuracy: 0.8535 - val_loss: 0.6974 - val_accuracy: 0.7531\n","Epoch 11/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.3871 - accuracy: 0.8668\n","Epoch 00011: val_loss improved from 0.69744 to 0.69415, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3872 - accuracy: 0.8667 - val_loss: 0.6942 - val_accuracy: 0.7593\n","Epoch 12/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8771\n","Epoch 00012: val_loss did not improve from 0.69415\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3588 - accuracy: 0.8772 - val_loss: 0.6966 - val_accuracy: 0.7608\n","Epoch 13/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8900\n","Epoch 00013: val_loss did not improve from 0.69415\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3313 - accuracy: 0.8902 - val_loss: 0.7019 - val_accuracy: 0.7648\n","------------------\n","Epoch 1/20\n","533/541 [============================>.] - ETA: 0s - loss: 1.5268 - accuracy: 0.3317\n","Epoch 00001: val_loss improved from inf to 1.39307, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.5253 - accuracy: 0.3333 - val_loss: 1.3931 - val_accuracy: 0.4625\n","Epoch 2/20\n","541/541 [==============================] - ETA: 0s - loss: 1.2187 - accuracy: 0.5269\n","Epoch 00002: val_loss improved from 1.39307 to 1.07630, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2187 - accuracy: 0.5269 - val_loss: 1.0763 - val_accuracy: 0.5845\n","Epoch 3/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.9622 - accuracy: 0.6324\n","Epoch 00003: val_loss improved from 1.07630 to 0.92280, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9616 - accuracy: 0.6326 - val_loss: 0.9228 - val_accuracy: 0.6441\n","Epoch 4/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.8251 - accuracy: 0.6874\n","Epoch 00004: val_loss improved from 0.92280 to 0.84330, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8251 - accuracy: 0.6875 - val_loss: 0.8433 - val_accuracy: 0.6782\n","Epoch 5/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.7277 - accuracy: 0.7320\n","Epoch 00005: val_loss improved from 0.84330 to 0.78577, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7276 - accuracy: 0.7320 - val_loss: 0.7858 - val_accuracy: 0.7053\n","Epoch 6/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.7629\n","Epoch 00006: val_loss improved from 0.78577 to 0.74655, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6492 - accuracy: 0.7633 - val_loss: 0.7466 - val_accuracy: 0.7245\n","Epoch 7/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.5835 - accuracy: 0.7933\n","Epoch 00007: val_loss improved from 0.74655 to 0.71953, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5835 - accuracy: 0.7935 - val_loss: 0.7195 - val_accuracy: 0.7384\n","Epoch 8/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.5258 - accuracy: 0.8146\n","Epoch 00008: val_loss improved from 0.71953 to 0.70333, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5258 - accuracy: 0.8148 - val_loss: 0.7033 - val_accuracy: 0.7467\n","Epoch 9/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.8332\n","Epoch 00009: val_loss improved from 0.70333 to 0.69436, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4800 - accuracy: 0.8334 - val_loss: 0.6944 - val_accuracy: 0.7509\n","Epoch 10/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8471\n","Epoch 00010: val_loss improved from 0.69436 to 0.69265, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4403 - accuracy: 0.8473 - val_loss: 0.6927 - val_accuracy: 0.7527\n","Epoch 11/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8602\n","Epoch 00011: val_loss did not improve from 0.69265\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4059 - accuracy: 0.8604 - val_loss: 0.6942 - val_accuracy: 0.7556\n","Epoch 12/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8706\n","Epoch 00012: val_loss did not improve from 0.69265\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3765 - accuracy: 0.8707 - val_loss: 0.7000 - val_accuracy: 0.7561\n","------------------\n","Epoch 1/20\n","533/541 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3343\n","Epoch 00001: val_loss improved from inf to 1.41279, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5333 - accuracy: 0.3357 - val_loss: 1.4128 - val_accuracy: 0.4621\n","Epoch 2/20\n","538/541 [============================>.] - ETA: 0s - loss: 1.2188 - accuracy: 0.5119\n","Epoch 00002: val_loss improved from 1.41279 to 1.08317, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2184 - accuracy: 0.5119 - val_loss: 1.0832 - val_accuracy: 0.5676\n","Epoch 3/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.9874 - accuracy: 0.6123\n","Epoch 00003: val_loss improved from 1.08317 to 0.95606, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9870 - accuracy: 0.6124 - val_loss: 0.9561 - val_accuracy: 0.6286\n","Epoch 4/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.8489 - accuracy: 0.6817\n","Epoch 00004: val_loss improved from 0.95606 to 0.87084, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8487 - accuracy: 0.6817 - val_loss: 0.8708 - val_accuracy: 0.6740\n","Epoch 5/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.7370 - accuracy: 0.7333\n","Epoch 00005: val_loss improved from 0.87084 to 0.80082, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7368 - accuracy: 0.7333 - val_loss: 0.8008 - val_accuracy: 0.7085\n","Epoch 6/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.6412 - accuracy: 0.7739\n","Epoch 00006: val_loss improved from 0.80082 to 0.74931, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6412 - accuracy: 0.7739 - val_loss: 0.7493 - val_accuracy: 0.7311\n","Epoch 7/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.8034\n","Epoch 00007: val_loss improved from 0.74931 to 0.71522, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5652 - accuracy: 0.8035 - val_loss: 0.7152 - val_accuracy: 0.7465\n","Epoch 8/20\n","541/541 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.8278\n","Epoch 00008: val_loss improved from 0.71522 to 0.69395, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5018 - accuracy: 0.8278 - val_loss: 0.6939 - val_accuracy: 0.7562\n","Epoch 9/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.4533 - accuracy: 0.8433\n","Epoch 00009: val_loss improved from 0.69395 to 0.67954, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4532 - accuracy: 0.8434 - val_loss: 0.6795 - val_accuracy: 0.7624\n","Epoch 10/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8579\n","Epoch 00010: val_loss improved from 0.67954 to 0.67153, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4120 - accuracy: 0.8579 - val_loss: 0.6715 - val_accuracy: 0.7672\n","Epoch 11/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8719\n","Epoch 00011: val_loss improved from 0.67153 to 0.67120, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3755 - accuracy: 0.8722 - val_loss: 0.6712 - val_accuracy: 0.7695\n","Epoch 12/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8829\n","Epoch 00012: val_loss did not improve from 0.67120\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3457 - accuracy: 0.8830 - val_loss: 0.6757 - val_accuracy: 0.7686\n","Epoch 13/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8928\n","Epoch 00013: val_loss did not improve from 0.67120\n","541/541 [==============================] - 4s 8ms/step - loss: 0.3194 - accuracy: 0.8927 - val_loss: 0.6822 - val_accuracy: 0.7722\n","------------------\n","Epoch 1/20\n","541/541 [==============================] - ETA: 0s - loss: 1.5268 - accuracy: 0.3248\n","Epoch 00001: val_loss improved from inf to 1.39117, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5268 - accuracy: 0.3248 - val_loss: 1.3912 - val_accuracy: 0.4861\n","Epoch 2/20\n","535/541 [============================>.] - ETA: 0s - loss: 1.1877 - accuracy: 0.5596\n","Epoch 00002: val_loss improved from 1.39117 to 1.03318, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.1862 - accuracy: 0.5599 - val_loss: 1.0332 - val_accuracy: 0.6044\n","Epoch 3/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.6573\n","Epoch 00003: val_loss improved from 1.03318 to 0.89160, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9204 - accuracy: 0.6574 - val_loss: 0.8916 - val_accuracy: 0.6637\n","Epoch 4/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.7828 - accuracy: 0.7118\n","Epoch 00004: val_loss improved from 0.89160 to 0.81750, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7823 - accuracy: 0.7121 - val_loss: 0.8175 - val_accuracy: 0.6952\n","Epoch 5/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.7500\n","Epoch 00005: val_loss improved from 0.81750 to 0.77245, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6885 - accuracy: 0.7501 - val_loss: 0.7725 - val_accuracy: 0.7124\n","Epoch 6/20\n","541/541 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.7809\n","Epoch 00006: val_loss improved from 0.77245 to 0.74566, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6145 - accuracy: 0.7809 - val_loss: 0.7457 - val_accuracy: 0.7243\n","Epoch 7/20\n","541/541 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8014\n","Epoch 00007: val_loss improved from 0.74566 to 0.72907, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5563 - accuracy: 0.8014 - val_loss: 0.7291 - val_accuracy: 0.7324\n","Epoch 8/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.5054 - accuracy: 0.8222\n","Epoch 00008: val_loss improved from 0.72907 to 0.72273, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5052 - accuracy: 0.8223 - val_loss: 0.7227 - val_accuracy: 0.7371\n","Epoch 9/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.8366\n","Epoch 00009: val_loss improved from 0.72273 to 0.72126, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4645 - accuracy: 0.8367 - val_loss: 0.7213 - val_accuracy: 0.7395\n","Epoch 10/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8502\n","Epoch 00010: val_loss did not improve from 0.72126\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4307 - accuracy: 0.8502 - val_loss: 0.7260 - val_accuracy: 0.7401\n","Epoch 11/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8609\n","Epoch 00011: val_loss did not improve from 0.72126\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3983 - accuracy: 0.8611 - val_loss: 0.7352 - val_accuracy: 0.7400\n","------------------\n","Epoch 1/20\n","537/541 [============================>.] - ETA: 0s - loss: 1.5219 - accuracy: 0.3276\n","Epoch 00001: val_loss improved from inf to 1.39448, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5213 - accuracy: 0.3282 - val_loss: 1.3945 - val_accuracy: 0.4435\n","Epoch 2/20\n","534/541 [============================>.] - ETA: 0s - loss: 1.2460 - accuracy: 0.4927\n","Epoch 00002: val_loss improved from 1.39448 to 1.12869, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2444 - accuracy: 0.4934 - val_loss: 1.1287 - val_accuracy: 0.5403\n","Epoch 3/20\n","536/541 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.5868\n","Epoch 00003: val_loss improved from 1.12869 to 0.98799, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.0259 - accuracy: 0.5874 - val_loss: 0.9880 - val_accuracy: 0.6105\n","Epoch 4/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.8838 - accuracy: 0.6577\n","Epoch 00004: val_loss improved from 0.98799 to 0.89672, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8830 - accuracy: 0.6581 - val_loss: 0.8967 - val_accuracy: 0.6532\n","Epoch 5/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.7779 - accuracy: 0.7036\n","Epoch 00005: val_loss improved from 0.89672 to 0.83762, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7778 - accuracy: 0.7037 - val_loss: 0.8376 - val_accuracy: 0.6803\n","Epoch 6/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.6947 - accuracy: 0.7415\n","Epoch 00006: val_loss improved from 0.83762 to 0.79267, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6947 - accuracy: 0.7415 - val_loss: 0.7927 - val_accuracy: 0.7022\n","Epoch 7/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.7757\n","Epoch 00007: val_loss improved from 0.79267 to 0.75768, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6170 - accuracy: 0.7759 - val_loss: 0.7577 - val_accuracy: 0.7204\n","Epoch 8/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.5529 - accuracy: 0.8024\n","Epoch 00008: val_loss improved from 0.75768 to 0.73578, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5527 - accuracy: 0.8025 - val_loss: 0.7358 - val_accuracy: 0.7309\n","Epoch 9/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.4986 - accuracy: 0.8253\n","Epoch 00009: val_loss improved from 0.73578 to 0.72018, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4984 - accuracy: 0.8254 - val_loss: 0.7202 - val_accuracy: 0.7404\n","Epoch 10/20\n","541/541 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.8422\n","Epoch 00010: val_loss improved from 0.72018 to 0.70953, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4510 - accuracy: 0.8422 - val_loss: 0.7095 - val_accuracy: 0.7491\n","Epoch 11/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8607\n","Epoch 00011: val_loss improved from 0.70953 to 0.70111, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4104 - accuracy: 0.8608 - val_loss: 0.7011 - val_accuracy: 0.7526\n","Epoch 12/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8735\n","Epoch 00012: val_loss improved from 0.70111 to 0.70100, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3743 - accuracy: 0.8739 - val_loss: 0.7010 - val_accuracy: 0.7575\n","Epoch 13/20\n","541/541 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8840\n","Epoch 00013: val_loss did not improve from 0.70100\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3441 - accuracy: 0.8840 - val_loss: 0.7042 - val_accuracy: 0.7593\n","Epoch 14/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8943\n","Epoch 00014: val_loss did not improve from 0.70100\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3170 - accuracy: 0.8945 - val_loss: 0.7119 - val_accuracy: 0.7610\n","------------------\n","Epoch 1/20\n","538/541 [============================>.] - ETA: 0s - loss: 1.5277 - accuracy: 0.3440\n","Epoch 00001: val_loss improved from inf to 1.40291, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.5274 - accuracy: 0.3443 - val_loss: 1.4029 - val_accuracy: 0.4636\n","Epoch 2/20\n","541/541 [==============================] - ETA: 0s - loss: 1.1801 - accuracy: 0.5593\n","Epoch 00002: val_loss improved from 1.40291 to 1.02749, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.1801 - accuracy: 0.5593 - val_loss: 1.0275 - val_accuracy: 0.6072\n","Epoch 3/20\n","533/541 [============================>.] - ETA: 0s - loss: 0.9258 - accuracy: 0.6444\n","Epoch 00003: val_loss improved from 1.02749 to 0.89830, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9250 - accuracy: 0.6449 - val_loss: 0.8983 - val_accuracy: 0.6519\n","Epoch 4/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.7993 - accuracy: 0.6987\n","Epoch 00004: val_loss improved from 0.89830 to 0.82945, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7988 - accuracy: 0.6989 - val_loss: 0.8294 - val_accuracy: 0.6839\n","Epoch 5/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.7389\n","Epoch 00005: val_loss improved from 0.82945 to 0.78333, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7085 - accuracy: 0.7393 - val_loss: 0.7833 - val_accuracy: 0.7061\n","Epoch 6/20\n","537/541 [============================>.] - ETA: 0s - loss: 0.6338 - accuracy: 0.7718\n","Epoch 00006: val_loss improved from 0.78333 to 0.75198, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.6341 - accuracy: 0.7717 - val_loss: 0.7520 - val_accuracy: 0.7207\n","Epoch 7/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.7980\n","Epoch 00007: val_loss improved from 0.75198 to 0.72734, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5681 - accuracy: 0.7979 - val_loss: 0.7273 - val_accuracy: 0.7319\n","Epoch 8/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.5163 - accuracy: 0.8199\n","Epoch 00008: val_loss improved from 0.72734 to 0.71522, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5161 - accuracy: 0.8201 - val_loss: 0.7152 - val_accuracy: 0.7375\n","Epoch 9/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.4687 - accuracy: 0.8368\n","Epoch 00009: val_loss improved from 0.71522 to 0.70568, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4690 - accuracy: 0.8367 - val_loss: 0.7057 - val_accuracy: 0.7439\n","Epoch 10/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.8533\n","Epoch 00010: val_loss improved from 0.70568 to 0.70425, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4273 - accuracy: 0.8532 - val_loss: 0.7043 - val_accuracy: 0.7477\n","Epoch 11/20\n","541/541 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.8675\n","Epoch 00011: val_loss did not improve from 0.70425\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3923 - accuracy: 0.8675 - val_loss: 0.7118 - val_accuracy: 0.7503\n","Epoch 12/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8751\n","Epoch 00012: val_loss did not improve from 0.70425\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3630 - accuracy: 0.8751 - val_loss: 0.7167 - val_accuracy: 0.7519\n","------------------\n","Epoch 1/20\n","534/541 [============================>.] - ETA: 0s - loss: 1.5219 - accuracy: 0.3247\n","Epoch 00001: val_loss improved from inf to 1.38406, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 1.5206 - accuracy: 0.3262 - val_loss: 1.3841 - val_accuracy: 0.4524\n","Epoch 2/20\n","540/541 [============================>.] - ETA: 0s - loss: 1.2102 - accuracy: 0.5305\n","Epoch 00002: val_loss improved from 1.38406 to 1.08255, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 1.2103 - accuracy: 0.5305 - val_loss: 1.0826 - val_accuracy: 0.5821\n","Epoch 3/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.9790 - accuracy: 0.6230\n","Epoch 00003: val_loss improved from 1.08255 to 0.94657, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.9788 - accuracy: 0.6230 - val_loss: 0.9466 - val_accuracy: 0.6342\n","Epoch 4/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.8466 - accuracy: 0.6789\n","Epoch 00004: val_loss improved from 0.94657 to 0.86495, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.8462 - accuracy: 0.6791 - val_loss: 0.8649 - val_accuracy: 0.6718\n","Epoch 5/20\n","538/541 [============================>.] - ETA: 0s - loss: 0.7475 - accuracy: 0.7212\n","Epoch 00005: val_loss improved from 0.86495 to 0.81135, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.7477 - accuracy: 0.7211 - val_loss: 0.8114 - val_accuracy: 0.6957\n","Epoch 6/20\n","540/541 [============================>.] - ETA: 0s - loss: 0.6658 - accuracy: 0.7580\n","Epoch 00006: val_loss improved from 0.81135 to 0.77352, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 0.6657 - accuracy: 0.7580 - val_loss: 0.7735 - val_accuracy: 0.7126\n","Epoch 7/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.5975 - accuracy: 0.7867\n","Epoch 00007: val_loss improved from 0.77352 to 0.74389, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5976 - accuracy: 0.7867 - val_loss: 0.7439 - val_accuracy: 0.7272\n","Epoch 8/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.5377 - accuracy: 0.8117\n","Epoch 00008: val_loss improved from 0.74389 to 0.72134, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.5377 - accuracy: 0.8117 - val_loss: 0.7213 - val_accuracy: 0.7371\n","Epoch 9/20\n","539/541 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.8296\n","Epoch 00009: val_loss improved from 0.72134 to 0.70638, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 0.4873 - accuracy: 0.8296 - val_loss: 0.7064 - val_accuracy: 0.7447\n","Epoch 10/20\n","536/541 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.8483\n","Epoch 00010: val_loss improved from 0.70638 to 0.69609, saving model to nn_model.h5\n","541/541 [==============================] - 4s 8ms/step - loss: 0.4415 - accuracy: 0.8485 - val_loss: 0.6961 - val_accuracy: 0.7515\n","Epoch 11/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8638\n","Epoch 00011: val_loss improved from 0.69609 to 0.69218, saving model to nn_model.h5\n","541/541 [==============================] - 4s 7ms/step - loss: 0.4021 - accuracy: 0.8640 - val_loss: 0.6922 - val_accuracy: 0.7561\n","Epoch 12/20\n","535/541 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8752\n","Epoch 00012: val_loss did not improve from 0.69218\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3692 - accuracy: 0.8753 - val_loss: 0.6935 - val_accuracy: 0.7580\n","Epoch 13/20\n","534/541 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.8853\n","Epoch 00013: val_loss did not improve from 0.69218\n","541/541 [==============================] - 4s 7ms/step - loss: 0.3394 - accuracy: 0.8857 - val_loss: 0.6984 - val_accuracy: 0.7593\n","------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4SVoW25lL8Nr"},"source":["# 최종 stacking ensemble"]},{"cell_type":"code","metadata":{"id":"ZzeojiuPCpDO","executionInfo":{"status":"ok","timestamp":1607339429572,"user_tz":-540,"elapsed":9369506,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["all_nn_train = np.hstack([gru_train1, gru_train2, \n","                        cnn_train1, cnn_train2,cnn_train3, cnn_train4,cnn_train5, cnn_train6,\n","                        nn_train1,nn_train2\n","                        ])\n","all_nn_test = np.hstack([gru_test1, gru_test2, \n","                        cnn_test1, cnn_test2,cnn_test3, cnn_test4,cnn_test5, cnn_test6,\n","                        nn_test1,nn_test2 \n","                        ])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZfgaBkwPbHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607339429975,"user_tz":-540,"elapsed":9369894,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"bce7af02-addd-4da9-827a-68b817c653c5"},"source":["# 최종 앙상블 데이터\n","cols_to_drop = ['index', 'text','tag_txt','ne_txt']\n","train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n","test_X = test_df.drop(cols_to_drop, axis=1).values\n","train_X = np.hstack([train_X,train_svd,train_svd2,train_cvec3,train_cvec4,train_tf5,train_tf6])\n","test_X = np.hstack([test_X,test_svd,test_svd2,test_cvec3,test_cvec4,test_tf5,test_tf6])\n","\n","f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n","f_train_X = np.round(f_train_X,4)\n","f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n","f_test_X = np.round(f_test_X,4)\n","print(f_train_X.shape, f_test_X.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(54879, 303) (19617, 303)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WAyZLnIBPbHr","executionInfo":{"status":"ok","timestamp":1607339429979,"user_tz":-540,"elapsed":9369894,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# 최종 앙상블입니다.\n","def cv_test(k_cnt=3, s_flag = False):\n","    rnd = 42\n","    if s_flag:\n","        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n","    else:\n","        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n","    test_pred = None\n","    weighted_test_pred = None\n","    org_train_pred = None\n","    avg_k_score = 0\n","    reverse_score = 0\n","    best_loss = 100\n","    best_single_pred = None\n","    for train_index, test_index in kf.split(f_train_X,train_Y):\n","        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n","        y_train, y_test = train_Y[train_index], train_Y[test_index]\n","        params = {\n","                'colsample_bytree': 0.7,\n","                'subsample': 0.8,\n","                'eta': 0.04,\n","                'max_depth': 3,\n","                'eval_metric':'mlogloss',\n","                'objective':'multi:softprob',\n","                'num_class':5,\n","                'tree_method':'gpu_hist'\n","        }\n","        \n","        d_train = xgb.DMatrix(X_train, y_train)\n","        d_valid = xgb.DMatrix(X_test, y_test)\n","        d_test = xgb.DMatrix(f_test_X)\n","        \n","        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n","        m = xgb.train(params, d_train, 2000, watchlist, \n","                        early_stopping_rounds=50,\n","                        verbose_eval=200)\n","        \n","        train_pred = m.predict(d_train)\n","        valid_pred = m.predict(d_valid)\n","        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n","        \n","        train_score = log_loss(y_train,train_pred)\n","        valid_score = log_loss(y_test,valid_pred)\n","        print('train log loss',train_score,'valid log loss',valid_score)\n","        avg_k_score += valid_score\n","        rev_valid_score = 1.0/valid_score\n","        reverse_score += rev_valid_score\n","        print('rev',rev_valid_score)\n","        \n","        if test_pred is None:\n","            test_pred = m.predict(d_test)\n","            weighted_test_pred = test_pred*rev_valid_score\n","            org_train_pred = tmp_train_pred\n","            best_loss = valid_score\n","            best_single_pred = test_pred\n","        else:\n","            curr_pred = m.predict(d_test)\n","            test_pred += curr_pred\n","            weighted_test_pred += curr_pred*rev_valid_score\n","            org_train_pred += tmp_train_pred\n","\n","            if valid_score < best_loss:\n","                print('BETTER')\n","                best_loss = valid_score\n","                best_single_pred = curr_pred\n","\n","    test_pred = test_pred / k_cnt\n","    test_pred = np.round(test_pred,4)\n","    org_train_pred = org_train_pred / k_cnt\n","    avg_k_score = avg_k_score/k_cnt\n","\n","    submiss=pd.read_csv(\"sample_submission.csv\")\n","    submiss['0']=test_pred[:,0]\n","    submiss['1']=test_pred[:,1]\n","    submiss['2']=test_pred[:,2]\n","    submiss['3']=test_pred[:,3]\n","    submiss['4']=test_pred[:,4]\n","    submiss.to_csv(\"xgb_{}.csv\".format(k_cnt),index=False)\n","    print(reverse_score)\n","    # weigthed\n","    submiss=pd.read_csv(\"sample_submission.csv\")\n","    weighted_test_pred = weighted_test_pred / reverse_score\n","    weighted_test_pred = np.round(weighted_test_pred,4)\n","    submiss['0']=weighted_test_pred[:,0]\n","    submiss['1']=weighted_test_pred[:,1]\n","    submiss['2']=weighted_test_pred[:,2]\n","    submiss['3']=weighted_test_pred[:,3]\n","    submiss['4']=weighted_test_pred[:,4]\n","    submiss.to_csv(\"weighted_{}.csv\".format(k_cnt),index=False)\n","    # best single\n","    submiss=pd.read_csv(\"sample_submission.csv\")\n","    weighted_test_pred = np.round(best_single_pred,4)\n","    submiss['0']=weighted_test_pred[:,0]\n","    submiss['1']=weighted_test_pred[:,1]\n","    submiss['2']=weighted_test_pred[:,2]\n","    submiss['3']=weighted_test_pred[:,3]\n","    submiss['4']=weighted_test_pred[:,4]\n","    submiss.to_csv(\"single_{}.csv\".format(k_cnt),index=False)\n","    \n","    # train log loss\n","    print('local average valid loss',avg_k_score)\n","    print('train log loss', log_loss(train_Y,org_train_pred))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIFxdKsRPbHs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607339784860,"user_tz":-540,"elapsed":9724757,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"6ec265b0-7fea-4ef7-e07e-da896ad29686"},"source":["cv_test(5, True)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[0]\ttrain-mlogloss:1.53927\tvalid-mlogloss:1.53965\n","Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n","\n","Will train until valid-mlogloss hasn't improved in 50 rounds.\n","[200]\ttrain-mlogloss:0.355055\tvalid-mlogloss:0.384927\n","[400]\ttrain-mlogloss:0.312747\tvalid-mlogloss:0.364205\n","[600]\ttrain-mlogloss:0.284918\tvalid-mlogloss:0.35637\n","[800]\ttrain-mlogloss:0.262457\tvalid-mlogloss:0.352282\n","[1000]\ttrain-mlogloss:0.24276\tvalid-mlogloss:0.349757\n","[1200]\ttrain-mlogloss:0.225228\tvalid-mlogloss:0.348044\n","[1400]\ttrain-mlogloss:0.209108\tvalid-mlogloss:0.346995\n","[1600]\ttrain-mlogloss:0.194379\tvalid-mlogloss:0.346386\n","Stopping. Best iteration:\n","[1655]\ttrain-mlogloss:0.190594\tvalid-mlogloss:0.346087\n","\n","train log loss 0.1871349243375515 valid log loss 0.34632040740935727\n","rev 2.887499490660916\n","[0]\ttrain-mlogloss:1.5395\tvalid-mlogloss:1.53957\n","Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n","\n","Will train until valid-mlogloss hasn't improved in 50 rounds.\n","[200]\ttrain-mlogloss:0.355507\tvalid-mlogloss:0.380995\n","[400]\ttrain-mlogloss:0.312393\tvalid-mlogloss:0.361945\n","[600]\ttrain-mlogloss:0.284494\tvalid-mlogloss:0.355617\n","[800]\ttrain-mlogloss:0.26189\tvalid-mlogloss:0.352161\n","[1000]\ttrain-mlogloss:0.242195\tvalid-mlogloss:0.350699\n","[1200]\ttrain-mlogloss:0.224271\tvalid-mlogloss:0.349654\n","[1400]\ttrain-mlogloss:0.208158\tvalid-mlogloss:0.349202\n","Stopping. Best iteration:\n","[1384]\ttrain-mlogloss:0.209383\tvalid-mlogloss:0.349093\n","\n","train log loss 0.20557773120822534 valid log loss 0.3493020445371943\n","rev 2.8628518373688423\n","[0]\ttrain-mlogloss:1.53935\tvalid-mlogloss:1.53961\n","Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n","\n","Will train until valid-mlogloss hasn't improved in 50 rounds.\n","[200]\ttrain-mlogloss:0.356397\tvalid-mlogloss:0.378724\n","[400]\ttrain-mlogloss:0.313819\tvalid-mlogloss:0.358692\n","[600]\ttrain-mlogloss:0.286271\tvalid-mlogloss:0.351231\n","[800]\ttrain-mlogloss:0.263686\tvalid-mlogloss:0.347515\n","[1000]\ttrain-mlogloss:0.243915\tvalid-mlogloss:0.345957\n","[1200]\ttrain-mlogloss:0.226218\tvalid-mlogloss:0.344888\n","[1400]\ttrain-mlogloss:0.210101\tvalid-mlogloss:0.344288\n","[1600]\ttrain-mlogloss:0.195372\tvalid-mlogloss:0.344015\n","Stopping. Best iteration:\n","[1564]\ttrain-mlogloss:0.197895\tvalid-mlogloss:0.343832\n","\n","train log loss 0.19437325999027433 valid log loss 0.34399667660108874\n","rev 2.9070048288857073\n","BETTER\n","[0]\ttrain-mlogloss:1.53924\tvalid-mlogloss:1.53941\n","Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n","\n","Will train until valid-mlogloss hasn't improved in 50 rounds.\n","[200]\ttrain-mlogloss:0.354988\tvalid-mlogloss:0.383003\n","[400]\ttrain-mlogloss:0.312654\tvalid-mlogloss:0.363651\n","[600]\ttrain-mlogloss:0.284667\tvalid-mlogloss:0.356896\n","[800]\ttrain-mlogloss:0.262017\tvalid-mlogloss:0.353234\n","[1000]\ttrain-mlogloss:0.242279\tvalid-mlogloss:0.351357\n","[1200]\ttrain-mlogloss:0.224574\tvalid-mlogloss:0.350385\n","[1400]\ttrain-mlogloss:0.208438\tvalid-mlogloss:0.349831\n","Stopping. Best iteration:\n","[1526]\ttrain-mlogloss:0.198906\tvalid-mlogloss:0.349491\n","\n","train log loss 0.19528938971349472 valid log loss 0.34956167770117647\n","rev 2.8607254850597554\n","[0]\ttrain-mlogloss:1.53927\tvalid-mlogloss:1.53968\n","Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n","\n","Will train until valid-mlogloss hasn't improved in 50 rounds.\n","[200]\ttrain-mlogloss:0.357018\tvalid-mlogloss:0.377241\n","[400]\ttrain-mlogloss:0.31501\tvalid-mlogloss:0.356866\n","[600]\ttrain-mlogloss:0.287003\tvalid-mlogloss:0.349596\n","[800]\ttrain-mlogloss:0.264154\tvalid-mlogloss:0.346261\n","[1000]\ttrain-mlogloss:0.244194\tvalid-mlogloss:0.344169\n","[1200]\ttrain-mlogloss:0.226426\tvalid-mlogloss:0.343249\n","Stopping. Best iteration:\n","[1227]\ttrain-mlogloss:0.224088\tvalid-mlogloss:0.34307\n","\n","train log loss 0.21985932133163738 valid log loss 0.34311937873165277\n","rev 2.9144375456044447\n","BETTER\n","14.432519187579667\n","local average valid loss 0.3464600369960939\n","train log loss 0.2177810932259759\n"],"name":"stdout"}]}]}